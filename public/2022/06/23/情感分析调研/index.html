<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>情感分析调研 | 喵喵虎冲呀</title><meta name="keywords" content="大三下,情感分析,调研"><meta name="author" content="MewTiger,nbspfjh@126.com"><meta name="copyright" content="MewTiger"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="情感分析基础 概念 自然语言处理中的情感分析是分析人类对某个目标对象所蕴含的观点，在业界一般称为观点挖掘（opinion mining），又因主要研究人类通过文字所表达的情感，也称为文本情感分析 。 其主要分析人类对一个目标对象所产生的情感极性、情绪、评价、态度。目标对象包括但不限于商品、服务、组织、个人、事件等等。根据不同场景又有不同任务，例如情绪识别、对话情感分析、情感对话生成、观点分析、观点">
<meta property="og:type" content="article">
<meta property="og:title" content="情感分析调研">
<meta property="og:url" content="http://81.69.237.168/2022/06/23/%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90%E8%B0%83%E7%A0%94/index.html">
<meta property="og:site_name" content="喵喵虎冲呀">
<meta property="og:description" content="情感分析基础 概念 自然语言处理中的情感分析是分析人类对某个目标对象所蕴含的观点，在业界一般称为观点挖掘（opinion mining），又因主要研究人类通过文字所表达的情感，也称为文本情感分析 。 其主要分析人类对一个目标对象所产生的情感极性、情绪、评价、态度。目标对象包括但不限于商品、服务、组织、个人、事件等等。根据不同场景又有不同任务，例如情绪识别、对话情感分析、情感对话生成、观点分析、观点">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://81.69.237.168/img/7.jpg">
<meta property="article:published_time" content="2022-06-23T04:35:17.000Z">
<meta property="article:modified_time" content="2022-06-30T06:03:24.238Z">
<meta property="article:author" content="MewTiger">
<meta property="article:tag" content="大三下">
<meta property="article:tag" content="情感分析">
<meta property="article:tag" content="调研">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://81.69.237.168/img/7.jpg"><link rel="shortcut icon" href="/img/favicon1.png"><link rel="canonical" href="http://81.69.237.168/2022/06/23/%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90%E8%B0%83%E7%A0%94/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://lf26-cdn-tos.bytecdntp.com/cdn/expire-1-M/font-awesome/6.0.0/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":false,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'null',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.奚黎曼.我爱你/js/flickr-justified-gallery-master/dist/fjGallery.js',
      css: 'https://cdn.奚黎曼.我爱你/js/flickr-justified-gallery-master/dist/fjGallery.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: true
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '情感分析调研',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-06-30 14:03:24'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" type="text/css" href="https://npm.elemecdn.com/js-heo@1.0.11/mainColor/heoMainColor.css"><link rel="stylesheet" type="text/css" href="https://npm.elemecdn.com/js-heo@1.0.11/categoryBar/categoryBar.css"><script async src="https://cdn.xn--5ts203a3u7b.xn--6qq986b3xl/hexo/categoryBar.js"></script><script defer src="https://cdn.xn--5ts203a3u7b.xn--6qq986b3xl/hexo/autofont.js"></script><link rel="stylesheet" href="https://cdn.奚黎曼.我爱你/dist/dist/css/index.801cd833.css"><link rel="stylesheet" href="https://cdn.xn--5ts203a3u7b.xn--6qq986b3xl/js/katex/katex.min.css"><script defer src="https://cdn.xn--5ts203a3u7b.xn--6qq986b3xl/js/katex/katex.min.js"></script><script defer src ="https://cdn.xn--5ts203a3u7b.xn--6qq986b3xl/js/katex/contrib/auto-render.min.js"><script src="//apps.bdimg.com/libs/jquery/2.1.4/jquery.min.js"></script><link rel="stylesheet" href="//at.alicdn.com/t/font_3438071_9ry2scekhk.css"/><link rel="stylesheet" href="//at.alicdn.com/t/font_3438071_ybyyxsdpzvc.css"/><link rel="stylesheet" href="https://cdn.xn--5ts203a3u7b.xn--6qq986b3xl/bber/swiper-bundle.min.css"><link rel="stylesheet" href="https://cdn.xn--5ts203a3u7b.xn--6qq986b3xl/bber/showbb_in_index.css"><!-- hexo injector head_end start --><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/assets/font-awesome-animation.min.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/tag_plugins.css" media="defer" onload="this.media='all'"><script src="https://npm.elemecdn.com/hexo-butterfly-tag-plugins-plus@latest/lib/assets/carousel-touch.js"></script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.2.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://mewtiger-1311904225.cos.ap-nanjing.myqcloud.com/post/7A4D4AF18D661DBAB5D69C3BE2A3F8F4.png" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">72</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">31</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">8</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/bb/"><i class="fa-fw fa-brands fa-rocketchat"></i><span> 说说</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li><li><a class="site-page child" href="/books/"><i class="fa-fw fa-solid fa-book"></i><span> 书籍</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/img/7.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">喵喵虎冲呀</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/bb/"><i class="fa-fw fa-brands fa-rocketchat"></i><span> 说说</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 清单</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li><li><a class="site-page child" href="/gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></li><li><a class="site-page child" href="/movies/"><i class="fa-fw fas fa-video"></i><span> 电影</span></a></li><li><a class="site-page child" href="/books/"><i class="fa-fw fa-solid fa-book"></i><span> 书籍</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">情感分析调研</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2022-06-23T04:35:17.000Z" title="发表于 2022-06-23 12:35:17">2022-06-23</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-06-30T06:03:24.238Z" title="更新于 2022-06-30 14:03:24">2022-06-30</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/">人工智能</a></span></div><div class="meta-secondline"></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="情感分析基础">情感分析基础</h2>
<h3 id="概念">概念</h3>
<p>自然语言处理中的情感分析是分析人类对某个目标对象所蕴含的观点，在业界一般称为<u>观点挖掘</u>（opinion mining），又因主要研究人类通过文字所表达的情感，也称为<u>文本情感分析</u> 。</p>
<p>其主要分析人类对一个目标对象所产生的情感极性、情绪、评价、态度。目标对象包括但不限于商品、服务、组织、个人、事件等等。根据不同场景又有不同任务，例如情绪识别、对话情感分析、情感对话生成、观点分析、观点摘要、主观分析、情感计算、评价分析等等</p>
<h3 id="粒度">粒度</h3>
<p>针对文本的情感分析分为：<code>词级别情感分析</code>、<code>句子/文档级情感分析</code>、<code>目标级情感分析</code>。其中<strong>词级别和句子级别</strong>的分析对象分别是一个词和整个句子的情感正负向，不区分句子中具体的目标，如实体或属性，相当于忽略了五要素中的<strong>实体</strong>和<strong>属性</strong>这两个要素。</p>
<h4 id="词级别">词级别</h4>
<p>即情感词典构建，研究的是如何给词赋予情感信息，如<code>庆典</code>对应的情感标签是<code>正面</code>。</p>
<h4 id="句子-文档级">句子/文档级</h4>
<p>研究如何给整个句子或篇章打情感标签，如<code>今天天气非常好</code>对应的情感标签是<code>正面</code>。</p>
<h4 id="目标级">目标级</h4>
<p>考虑了具体的目标，该目标可以是<code>实体</code>、<code>某个实体的属性</code>或<code>实体加属性</code>的组合。具体可分为三种</p>
<h5 id="1）-Target-no-aspect-based-sentiment-analysis-TN-ABSA">1） Target no aspect based sentiment analysis (TN-ABSA)</h5>
<p>TN-ABSA 的分析对象是文本中出现的<code>实体</code>的情感正负向，这种情况下没有属性的概念，只有实体。</p>
<table>
<thead>
<tr>
<th>评论</th>
<th style="text-align:center">Apple Music</th>
<th style="text-align:center">网易云</th>
</tr>
</thead>
<tbody>
<tr>
<td>Apple Music比网易云干净多了。</td>
<td style="text-align:center">🙂</td>
<td style="text-align:center">😣</td>
</tr>
</tbody>
</table>
<h5 id="2）-Target-grounded-aspect-based-sentiment-analysis-TG-ABSA">2） Target-grounded aspect based sentiment analysis (TG-ABSA)</h5>
<p>TG-ABSA 的分析对象是给定某一个实体的情况下该实体给定<code>属性集合下的各个属性</code>的情感分析</p>
<table>
<thead>
<tr>
<th>评论</th>
<th style="text-align:center">材质</th>
<th style="text-align:center">大小</th>
<th style="text-align:center">价格</th>
</tr>
</thead>
<tbody>
<tr>
<td>这款助力带非常好用，牛皮材质经久耐用，大小刚好合适，就是价格偏高。</td>
<td style="text-align:center">🙂</td>
<td style="text-align:center">🙂</td>
<td style="text-align:center">😣</td>
</tr>
</tbody>
</table>
<h5 id="3）-Target-aspect-based-sentiment-analysis-T-ABSA">3） Target aspect based sentiment analysis (T-ABSA)</h5>
<p>T-ABSA 的分析对象是文本中出现的<code>实体和属性组合</code></p>
<table>
<thead>
<tr>
<th>评论</th>
<th style="text-align:center">AM外语曲库</th>
<th style="text-align:center">网易云内地曲库</th>
</tr>
</thead>
<tbody>
<tr>
<td>Apple Music外语曲库大，但网易云内地曲库大。</td>
<td style="text-align:center">🙂</td>
<td style="text-align:center">🙂</td>
</tr>
</tbody>
</table>
<h3 id="情感分析要素">情感分析要素</h3>
<p>情感分析主要有五个要素（<code>entity/实体，aspect/属性，opinion/观点，holder/观点持有者，time/时间</code>），其中实体和属性合并称为对象 (target)。而当前研究中一般都不考虑情感分析五要素中的观点持有者和时间，故后文中的讨论都不考虑这两个因素。</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://mewtiger-1311904225.cos.ap-nanjing.myqcloud.com/post/image-20220623153831150.png" alt="image-20220623153831150"></p>
<h4 id="情感分析五元素">情感分析五元素</h4>
<p>举例：“<u>我觉得陀斯妥耶夫斯写的书很好看</u>”</p>
<ol>
<li><strong>对象</strong>：表示对哪个对象有观点或情感评价，即「<strong>陀思妥耶夫斯基写的书</strong>」。</li>
<li><strong>观点</strong>：观点有多种表示法，具体下面会提及，这里暂用极性来说明，即「<strong>正面</strong>」。</li>
<li><strong>持有者</strong>：即谁表达了该观点，即「<strong>我</strong>」。如果在文本中没有指明具体持有者，通常需要借助于文本来源信息。记录持有者的意义在于，在后续分析中往往有针对人群观点评估的需要，结合用户画像可以产生很多不同的分析维度，例如分析不同地域的情感差异。</li>
<li><strong>时间</strong>：即观点产生的时间，通常文本中不会显式写出观点产生的时间，也需要借助于文本来源，例如微博的发送时间。观点时间也有利于后续的分析，不同的时间段人群对某类事物的评价也有很大差异。</li>
</ol>
<h4 id="对象结构化表示">对象结构化表示</h4>
<p>情感对象/评价对象一般用三个元素来表示：<code>(实体类型，实体，实体方面) - (category, entity, aspect)</code>。元素之间本身具有层级关系，从左到右概念越来越细化，其中<strong>实体类型</strong>本身也可以有多个层级，而<strong>实体方面</strong>往下也可以继续拆分成多个子方面</p>
<p><strong>举例</strong>：“<u>《罪与罚》的心理描写很真切</u>”</p>
<ol>
<li><strong>实体类型</strong>：实体类型为陀思妥耶夫斯基的著作，往上仍有更高的实体类型层级，如<code>书籍 → 俄国文学 → 小说</code></li>
<li><strong>实体</strong>：《罪与罚》这本书</li>
<li><strong>实体方面</strong>：具体实体的某个属性或子属性，这里为<code>《罪与罚》—— 心理描写</code></li>
</ol>
<h4 id="情感结构化表示">情感结构化表示</h4>
<p>情感分析要素（对象，观点，持有者，时间）中的<strong>观点</strong>有多种表示方法，可以用三个元素来表示：<code>(类型，极性/强度，措辞) - (type,orientation/intensity,opinion terms)</code>。</p>
<ol>
<li><strong>类型</strong>：可分为理性情感与情绪情感。
<ol>
<li><strong>理性情感</strong>一般指的是通用情感，即没有<strong>具体情绪类别</strong>的情感。如“<u>《罪与罚》的心理描写很真切</u>”</li>
<li><strong>情绪情感</strong>有具体的情绪标签，并且情绪的定义有时也跟具体业务强相关。如“<u>我喜欢这本书</u>”</li>
</ol>
</li>
<li><strong>极性/强度</strong>：表示对应情感类型的极性或强度，可以用数值表示法（0-10）或类别表示法（正面、负面、中性）。</li>
<li><strong>措辞</strong>：表示具体情感的词组、短语或短句，例如“<u>仰卧起坐效果一般，还容易伤颈椎</u>”，对应的情感词为<code>一般</code>。</li>
</ol>
<h3 id="作用-2">作用</h3>
<p>由于情感分析具有众多的应用场景，如商品评论的分析、舆情分析等等，因此情感分析受到工业界的广泛关注，已成为自然语言处理研究应用落地的重要体现。</p>
<p>另外，情感分析还在社会学、经济学、管理学等领域都显示出重要的研究意义和广泛的应用场景，这些需求对情感分析不断提出了更高的要求，从而推动了情感分析研究的内涵和外延不断拓展和深入。</p>
<h2 id="方法">方法</h2>
<p>情感分析的主要方法分为：基于<code>情感词典</code>的情感分析方法、基于<code>机器学习</code>的情感分析方法、基于<code>深度学习</code>的情感分析方法</p>
<h3 id="情感词典方法">情感词典方法</h3>
<p>基于词典的文本情感分析技术由于构建的词典往往<strong>只针对某个领域</strong>，对于跨领域情感分析的效果不够好，而且词典中的情感词可能不够丰富，对于短文本和特定领域文本进行情感分析的效果更好。因此，对于长文本来说，更好的解决方法是利用机器学习方法。</p>
<h4 id="人工构建情感字典">人工构建情感字典</h4>
<p>人工构建词典是抓取数据之后多次进行人工标注，根据情感表达将词语进行正负向和强弱程度区分。比较典型的中文情感词典是王勇等对微博中的句子构建的极性词典。台湾大学的中文情感极性词典（NTUSD）、知网情感词典 HowNet等。</p>
<p>人工构建词典在扩充词条信息和便利性方面有一定的优势，但是大大增加了人工开销，并且设计的范围有限，不适合跨领域研究。</p>
<h4 id="自动构建情感字典">自动构建情感字典</h4>
<p>第一种是<strong>基于知识库的方法</strong>，是对上面的人工构建情感词典进行的拓展，加入名词动词副词，使情感词更加全面。</p>
<blockquote>
<p>完备的语义知识库, 能够快速构建通用性较强的情感词典, 对词典的精度要求不高的情况下, 这种方法较为实用。中文语义知识库的不足以及领域的限制使得该方法在构建面向单一领域的情感词典中表现不佳。</p>
</blockquote>
<p>第二种是<strong>基于语料库的方法</strong>，利用相关领域的大量语料和相关度的计算规则，结合机器学习的方法，自动统计情感词的情感极性，自动构建情感词典。</p>
<blockquote>
<p>语料库相对于语义知识库而言, 其优点是容易获得且数量充足, 构建的词典在语料所属的领域内表现较好, 但是构建的成本较高, 需要对语料进行预处理, 另外, 所构建的词典的准确率相对不高。</p>
</blockquote>
<p>第三种是<strong>知识库和语料库结合</strong>的方法，通过将扩充的情感知识库和特定领域的语料库结合，使构成的情感词典更加丰富。</p>
<h4 id="小结-2">小结</h4>
<p>优点：</p>
<ol>
<li>能有效反应文本的结构特征，易于理解</li>
<li>在情感词数量充足时分类效果明显</li>
</ol>
<p>缺点：</p>
<ol>
<li>没有突破情感词典的限制，要对情感词典不断扩充，使得文本情感判断的准确率不高</li>
</ol>
<h3 id="基于传统机器学习的情感分析方法">基于<strong>传统机器学习</strong>的情感分析方法</h3>
<h4 id="朴素贝叶斯">朴素贝叶斯</h4>
<p>基于朴素贝叶斯的方法是通过计算概率来对文本情感进行分类的，适合增量式训练，特点是<code>算法简单</code></p>
<h5 id="1）-贝叶斯公式">1） 贝叶斯公式</h5>
<p>首先计算单个<code>唯一词</code>属于各情感类别的概率</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mi>i</mi></msub><mi mathvariant="normal">∣</mi><mi>c</mi><mi>l</mi><mi>a</mi><mi>s</mi><mi>s</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi>f</mi><mi>r</mi><mi>e</mi><mi>q</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mi>i</mi></msub><mo separator="true">,</mo><mi>c</mi><mi>l</mi><mi>a</mi><mi>s</mi><mi>s</mi><mo stretchy="false">)</mo></mrow><msub><mi>N</mi><mrow><mi>c</mi><mi>l</mi><mi>a</mi><mi>s</mi><mi>s</mi></mrow></msub></mfrac></mrow><annotation encoding="application/x-tex">P(w_{i}|class) = \frac{freq(w_{i},class)}{N_{class}}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord mathnormal">c</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">a</span><span class="mord mathnormal">ss</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:2.263em;vertical-align:-0.836em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.109em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight">ss</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mord mathnormal">re</span><span class="mord mathnormal" style="margin-right:0.03588em;">q</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">c</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">a</span><span class="mord mathnormal">ss</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.836em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
<ul>
<li><strong>P(w<sub>i</sub> | class)</strong> ：词 <strong>w<sub>i</sub></strong> 属于类别 <strong>class</strong>的概率</li>
<li><strong>freq(w<sub>i</sub> | class)</strong> ：词 <strong>w<sub>i</sub></strong> 在类别为<strong>class</strong>的语料库中出现的频率</li>
<li><strong>N<sub>class</strong></sub>： 类别为<strong>class</strong>的语料库中 <strong>唯一词总数</strong></li>
</ul>
<h6 id="例子">例子</h6>
<p>有如下四条推文，两条属于积极推文，另外两条属于消极推文</p>
<p><strong>Positive</strong>: ‘I am happy because I am learning NLP,’ ‘I am happy, not sad.’</p>
<p><strong>Negative</strong>: ‘I am sad, I am not learning NLP,’ ‘I am sad, not happy.’</p>
<ul>
<li>
<p>I</p>
<ul>
<li>
<p>p( ‘I’ | positive) = 3/13 = 0.24</p>
</li>
<li>
<p>p( ‘I’ | negative) = 3/12 = 0.25</p>
</li>
</ul>
</li>
<li>
<p>am</p>
<ul>
<li>
<p>p( ‘am’ | positive) = 3/13 = 0.24</p>
</li>
<li>
<p>p( ‘am’ | negative) = 3/12 = 0.25</p>
</li>
</ul>
</li>
</ul>
<p>通过类似的计算， 可得到所有唯一词属于各类别的概率表</p>
<table>
<thead>
<tr>
<th style="text-align:center">Word</th>
<th style="text-align:center">pos</th>
<th style="text-align:center">neg</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">i</td>
<td style="text-align:center">0.24</td>
<td style="text-align:center">0.25</td>
</tr>
<tr>
<td style="text-align:center">am</td>
<td style="text-align:center">0.24</td>
<td style="text-align:center">0.25</td>
</tr>
<tr>
<td style="text-align:center">|</td>
<td style="text-align:center">|</td>
<td style="text-align:center">|</td>
</tr>
<tr>
<td style="text-align:center">|</td>
<td style="text-align:center">|</td>
<td style="text-align:center">|</td>
</tr>
<tr>
<td style="text-align:center">not</td>
<td style="text-align:center">0.08</td>
<td style="text-align:center">0.17</td>
</tr>
<tr>
<td style="text-align:center">Sum</td>
<td style="text-align:center">1</td>
<td style="text-align:center">1</td>
</tr>
</tbody>
</table>
<h5 id="2）-拉普拉斯平滑-Laplace-Smoothing">2） 拉普拉斯平滑 (Laplace Smoothing)</h5>
<p>如果某个唯一词仅在特定类中出现，那它在其他类中出现的概率就会为0。在之后的朴素贝叶斯中就会导致分母为0情况的出现，因此引入拉普拉斯平滑。</p>
<h5 id="3）-朴素贝叶斯">3） 朴素贝叶斯</h5>
<p>每个单词对情感产生的影响，可以通过正负两类单词出现的概率之比来计算</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mfrac><mrow><mi>P</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mi>i</mi></msub><mi mathvariant="normal">∣</mi><mi>p</mi><mi>o</mi><mi>s</mi><mo stretchy="false">)</mo></mrow><mrow><mi>P</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mi>i</mi></msub><mi mathvariant="normal">∣</mi><mi>n</mi><mi>e</mi><mi>g</mi><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{P(w_{i}|pos)}{P(w_{i}|neg)}
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.363em;vertical-align:-0.936em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord mathnormal">n</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mclose">)</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord mathnormal">p</span><span class="mord mathnormal">os</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span></p>
<p>而整条推文的情绪就可以使用各唯一词的朴素贝叶斯成绩。同时为了防止数据不平衡以及因此产生的偏差，因此还需要引入<code>先验因子</code>。同时为了避免结果过大或过小，对结果取对数。</p>
<p><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy="false">(</mo><mfrac><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>p</mi><mi>o</mi><mi>s</mi><mo stretchy="false">)</mo></mrow><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>n</mi><mi>e</mi><mi>g</mi><mo stretchy="false">)</mo></mrow></mfrac><munderover><mo>∏</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>m</mi></munderover><mfrac><mrow><mi>P</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mi>i</mi></msub><mi mathvariant="normal">∣</mi><mi>p</mi><mi>o</mi><mi>s</mi><mo stretchy="false">)</mo></mrow><mrow><mi>P</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mi>i</mi></msub><mi mathvariant="normal">∣</mi><mi>n</mi><mi>e</mi><mi>g</mi><mo stretchy="false">)</mo></mrow></mfrac><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">log(\frac{P(pos)}{P(neg)}
\prod_{i=1}^{m}
\frac{P(w_{i}|pos)}{P(w_{i}|neg)})
</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.9291em;vertical-align:-1.2777em;"></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal">n</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mclose">)</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathnormal">p</span><span class="mord mathnormal">os</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.6514em;"><span style="top:-1.8723em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.05em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∏</span></span></span><span style="top:-4.3em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">m</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.2777em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord mathnormal">n</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mclose">)</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em;">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0269em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord mathnormal">p</span><span class="mord mathnormal">os</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mclose">)</span></span></span></span></span></p>
<p>若对数似然值大于0，则表明该句子是积极的。</p>
<h5 id="4）-缺点">4） 缺点</h5>
<ol>
<li>
<p>朴素贝叶斯在贝叶斯公式的基础上做了独立同分布的假设</p>
</li>
<li>
<p>该方法对输入数据的表达形式很敏感，而且需要计算先验概率，因此会在分类决策方面存在错误率</p>
</li>
</ol>
<h4 id="临近算法（KNN">临近算法（KNN)</h4>
<p>KNN算法是常见的分类算法之一，属于有监督学习中的分类算法，全称为K-Nearest Neighbor。</p>
<p>KNN算法的工作方法是将测试集中的每个特征与样本集中的特征进行比较，然后提取前k个最相似的数据标签，比较出现的频率，最后将频率最高的类别作为分类的结果输出。</p>
<p>步骤：</p>
<ol>
<li>预处理（文本向量化、特征选择）</li>
<li>计算训练集中的点与当前点之间的距离;</li>
<li>按照距离递增次序排序;</li>
<li>选取与当前点距离最小的 k 个点;</li>
<li>确定前 k 个点所在类别的出现频率;</li>
<li>返回前 k个点出现频率最高的类别作为当前点的预测分类。</li>
</ol>
<p>注意点：</p>
<ol>
<li><strong>k的选择</strong>：k太小，分类易受噪点影响；k太大，会包含太多其他类别的点</li>
<li><strong>权</strong>：越近的样本越可信，概率越大的样本越可信</li>
<li><strong>距离计算公式</strong>：在文本分类计算相似度中，<strong>余弦</strong>优于欧式</li>
</ol>
<h4 id="支持向量机（SVM）">支持向量机（SVM）</h4>
<p>支持向量机(SVM)是一种基于统计学习理论的机器学习算法。它基于线性分类器的原理首先由Vapnik提出的。SVM可以用于解决线性与非线性样本分类,其核心思想是将低维空间线性不可分的样本点通过核函数映射至高维特征空间中,然后在特征空间中构造出<strong>最优分类超平面</strong>,这时数据在高维空间也可以被超平面分割,从而变得线性可分，并且各个样本与超平面的举例应保持最大。</p>
<p>结果表明，基于Boosting算法的SVM混合情绪分析模型，<strong>性能显著优于单独的SVM模型</strong>。</p>
<p>基于SVM的文本情感分析方法被认为是最好的情感分析方法，该方法泛化错误率低，计算开销不大，而且对于训练样本较小的文本可以得到很好的情感分析效果，对高维数据的处理效果良好，能够得到较低的错误率，但该方法对参数调节和核函数的选择敏感。</p>
<h4 id="小结-3">小结</h4>
<p>优点：</p>
<ol>
<li>能根据<strong>情感特征的 选取</strong>以及<strong>情感分类器的组合</strong>对文本的情感进行分类</li>
</ol>
<p>缺点：</p>
<ol>
<li>不能充分利用<code>上下文文本</code>的语境信息，影响分类准确性</li>
<li><code>数据量大</code>时完成分类任务的效率和质量低</li>
</ol>
<p>下表为基于机器学习方法的情感分析的实验结果</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://mewtiger-1311904225.cos.ap-nanjing.myqcloud.com/post/image-20220623210857203.png" alt="image-20220623210857203"></p>
<h3 id="基于深度学习的情感分析方法">基于<strong>深度学习</strong>的情感分析方法</h3>
<h4 id="单一神经网络的情感分析方法">单一神经网络的情感分析方法</h4>
<p>典型的神经网络学习方法有：<code>卷积神经网络</code>（Convolutional Neural Network，CNN）、<code>递 归 神 经 网 络</code> （Recurrent Neural Network，RNN）、<code>长短时记忆</code>（Long Short-Term Memory，LSTM）网络等。</p>
<p>和使用基于情感词典和传统机器学习的情感分析方法相比，采用神经网络的方法在文本特征学习方面有显著优势，能<code>主动学习特征</code>，并对文本中的词语的信息<code>主动保留</code>，从而更好地提取到相应词语的语义信息，来有效实现文本的情感分类。</p>
<h4 id="混合（组合、融合）神经网络的情感分析方法">混合（组合、融合）神经网络的情感分析方法</h4>
<p>除了对单一神经网络的方法的研究之外，有不少学者在考虑了不同方法的优点后将这些方法进行组合和改进，并将其用于情感分析方面。</p>
<h4 id="引入注意力机制的情感分析">引入注意力机制的情感分析</h4>
<p>引入注意力机制，能够更好地捕获上下文相关信息，提取语义信息，防止重要信息的丢失，可以有效提高文本情感分类的准确率。现阶段的研究更多的是通过对预训练模型的微调和改进，从而更有效地提升实验的效果。</p>
<h4 id="使用预训练模型的情感分析">使用预训练模型的情感分析</h4>
<p>预训练模型是指用数据集已经训练好的模型。通过对预训练模型的微调，可以实现较好的情感分类结果，因此最新的方法大多是使用预训练模型，最新的预训练模型有：ELMo、BERT、XL-NET、ALBERT 等。</p>
<h4 id="小结-4">小结</h4>
<p><strong>优点</strong>：</p>
<ol>
<li>能充分利用上下文文本的语境</li>
<li>能主动学习文本特征，保留文本中词语的顺序信息，从而提取到相关词语的语义信息，来实现文本的情感分类。</li>
<li>通过深层网络模型学习数据中的关键信息，来反映数据的特征，从而提升学习的性能。</li>
<li>通过和传统方法相比，使用语言模型预训练的方法充分利用了大规模的单语语料，可以对一词多义进行建模，有效缓解对模型结构的依赖问题。</li>
</ol>
<p><strong>缺点</strong>：</p>
<ol>
<li>基于深度学习的方法需要大量数据支撑，不适合小规模数据集</li>
<li>算法训练时间取决于神经网络的深度和复杂度，一般花费时间较长</li>
</ol>
<h2 id="相关任务">相关任务</h2>
<h3 id="方面情感分析（ABSA）">方面情感分析（ABSA）</h3>
<p><strong>基于方面的情感分析</strong>(aspect-based sentiment analysis, ABSA)是一个重要的细粒度情感分析问题，它从相关文本识别出文本中某个相关<code>文本项</code>(text item)的情感元素。<strong>[34]</strong></p>
<p>它有4个关键情感元素分别为：</p>
<p>1.<strong>方面项</strong>(Aspect Term)：任意一个实体或实体的任意一方面</p>
<p>2.<strong>方面类别</strong>(Aspect Category)：方面项所属的类别，<strong>提前预定义</strong></p>
<p>3.<strong>观点项</strong>(Opinoin Term)：对方面项的观点描述（当观点表达争对实体的整体时，成为<code>GENERAL</code>）。</p>
<p>4.<strong>情感极性</strong>(Sentiment Polarity)：对整句话大体的情感极性描述</p>
<table>
<thead>
<tr>
<th>例句</th>
<th style="text-align:center">直立划船是很危险的</th>
</tr>
</thead>
<tbody>
<tr>
<td>Aspect Term</td>
<td style="text-align:center">直立划船</td>
</tr>
<tr>
<td>Aspect Category</td>
<td style="text-align:center">动作</td>
</tr>
<tr>
<td>Opinion Term</td>
<td style="text-align:center">危险</td>
</tr>
<tr>
<td>Sentiment Polarity</td>
<td style="text-align:center">消极</td>
</tr>
</tbody>
</table>
<h3 id="情绪识别（Emotion-Detection）">情绪识别（Emotion Detection）</h3>
<p>一般对情感类型进行讨论时指的都是理性情感，但在有些场景下需要更细粒度的情感类型，称为情绪情感（emotional sentiment），如：<u>开心、难受、沮丧</u>等。该任务目前有两个难点：</p>
<p>1.情绪<strong>分类较多</strong>，且不同情绪存在相似之处，例如<u>高兴和惊喜</u>、<u>生气和厌恶</u>等</p>
<p>2.情绪情感较为<strong>主观</strong>，不同人对同一段文本会有不同的感受，导致数据标注<code>难度</code>大，且数据<code>质量</code>可能不高。</p>
<h3 id="对话相关">对话相关</h3>
<h4 id="对话情感分析">对话情感分析</h4>
<p><strong>对话情感分析</strong>与情绪识别任务较为相似，主要区别在于分析的对象是<code>上下文强依赖</code>的对话，<strong>目标</strong>是识别对话中每一轮句子的情感。对话场景中关注的更多是<code>情绪情感</code>类型，即每一轮的具体情绪。</p>
<p>多轮对话包括非常多的场下问背景信息，一个<strong>理想的对话模型</strong>应该包括<u>对话者性格特征、对话主题、对话内容、对话者状态、对话者意图、对话者情感状态</u>。但实际情况中往往得不到如此多的上下文信息，以及可利用的数据集。因此最近的一些工作开始考虑如何<strong>利用更多信息建模</strong>，比如说话人的状态、对话人之间情绪影响，或者直接将所有可获得的 context 信息作为特征，用<code>序列标注模型</code>来解决。</p>
<h4 id="情感对话生成">情感对话生成</h4>
<p>情感在对话中另一个典型应用是对话生成，目标是生成具有情感特征的对话。根据生成过程中<strong>是否需要明确指定情绪情感</strong>，该任务可分成两种类型：</p>
<p>1.待生成回复的<strong>情绪类型已经确定</strong>，模型需要生成具有该指定情绪的对话。即<u>输入是（对话上文，指定情绪类型），输出是（蕴含该情绪的回复）</u></p>
<p>2.<strong>不给定具体情绪类型</strong>，根据上文直接生成具有合适情绪特征的回复。该方法认为指定情绪在实际应用中是<code>不现实</code>的，情绪回复应该已经隐含在整体对话主题和上下文之中了。即<u>输入是（对话上文），输出是（蕴含某情绪的回复）</u>。</p>
<h3 id="观点摘要">观点摘要</h3>
<p>观点摘要是观点挖掘<strong>结构化</strong>的关键任务，其目标是将一群人的多个观点进行<code>结构化</code>提取和展示，即观点摘要是<strong>一群观点</strong>的结构化表示。由于结构化需要比较细的粒度，一般用于 Aspect 粒度的摘要。</p>
<p>观点摘要有以下<strong>特点</strong>：</p>
<p>1.对一群人的观点进行总结</p>
<p>2.聚焦于具体的观点对象（实体/方面）</p>
<p>3.需要显示对象的情感评价</p>
<p>4.结构化</p>
<p>5.量化</p>
<p>6.从众多观点中总结出主要观点</p>
<p><code>商品评价</code>是观点摘要最常见的应用场景</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://mewtiger-1311904225.cos.ap-nanjing.myqcloud.com/post/image-20220627213922859.png" alt="image-20220627213922859"></p>
<h3 id="立场识别（Stance-Detection）">立场识别（Stance Detection）</h3>
<p>立场识别的目标是识别出一段文本的作者对一个对象，是否有<strong>支持、反对或中立</strong>的立场。立场识别一般用在社交媒体等适合用户表达观点的网站，作用域一般是<strong>经常有争议的对象</strong>，例如国会辩论、公司内部议题、在线论坛上的讨论、社交媒体的热点讨论等。</p>
<p>立场识别有如下特点，也正是这些特点导致了立场识别较高的难度。</p>
<p>1.Target 对象可能没有显示在文本中</p>
<ul>
<li>Target：拜登</li>
<li>Tweet：我认为特朗普在任期间做了很多愚蠢的决策。</li>
<li>上面 Tweet 虽然没有提拜登，但由于拜登和特朗普是对立关系，系统需要推断出作者<strong>支持</strong>拜登。</li>
</ul>
<p>2.积极情感与支持并不等价，如上文虽是消极情感，但表达了对拜登的支持</p>
<h3 id="反讽识别">反讽识别</h3>
<p>在社交媒体上经常会有“阴阳怪气”的出现。而反讽识别就是判断一个句子是否有讽刺、反讽的含义。从一句话的字面含义很难判断是否有反讽意思，往往和<strong>上下文强关联</strong>，所以反讽任务一般都比较难，属于<code>深层次语义理解</code>的一类。在实际应用里，能否真正判断一句话是否有讽刺意图，一般需要参考下面 context 信息：</p>
<ol>
<li>
<p>常识</p>
</li>
<li>
<p>说话人个性特征</p>
</li>
<li>
<p>对话上下文</p>
</li>
<li>
<p>多模态信息，例如说话人表情、肢体动作等</p>
</li>
</ol>
<p>目前关于反讽任务的工作一般分为两类：</p>
<p>1.将其当成分类任务，重点是如何有效的引入合适的 context 信息</p>
<p>2.利用反讽句子中的<code>短语意义不一致性</code>，例如“<u>这电影太精彩了让我睡得很香</u>”，其中含有矛盾的短语（电影太精彩，睡得很香）</p>
<h3 id="情感原因推理">情感原因推理</h3>
<p>有时，除了需要抽取对象的相应情感，还需要抽取引起该情感的原因，以辅助后续更复杂的因果关系。此时情感多元组就变成了（对象，观点，原因，持有者，时间）</p>
<p>情感原因推理是强依赖上下文的任务，系统需要对情感和原因之间的关系进行建模，目前该任务能用的数据集比较少，很多研究者都在积极推动数据集的建设。</p>
<table>
<thead>
<tr>
<th>例句</th>
<th style="text-align:center">这电影太难看了，感情戏加得莫名其妙</th>
</tr>
</thead>
<tbody>
<tr>
<td>对象</td>
<td style="text-align:center">电影</td>
</tr>
<tr>
<td>情感</td>
<td style="text-align:center">负面</td>
</tr>
<tr>
<td>情感原因</td>
<td style="text-align:center">感情戏</td>
</tr>
</tbody>
</table>
<h2 id="现状、问题和方向">现状、问题和方向</h2>
<h3 id="发展与现状">发展与现状</h3>
<h4 id="发展">发展</h4>
<div class="timeline purple"><div class='timeline-item headline'><div class='timeline-item-title'><div class='item-circle'><p><strong>Foundations</strong></p>
</div></div></div><div class='timeline-item'><div class='timeline-item-title'><div class='item-circle'><p>Wiebe</p>
</div></div><div class='timeline-item-content'><p><strong>Private States</strong>, 1994</p>
<p><strong>Subjectivity Analysis</strong>, 1999</p>
</div></div></div>
<div class="timeline purple"><div class='timeline-item headline'><div class='timeline-item-title'><div class='item-circle'><p><strong>SA on reviews</strong></p>
</div></div></div><div class='timeline-item'><div class='timeline-item-title'><div class='item-circle'><p>Bag of word &amp; Syntactic Rules,Turney 2002</p>
</div></div><div class='timeline-item-content'><p>该研究第一次根据评论的情感取向对评论进行分类。其通过列举几个<strong>句法规则</strong>来归纳短语级的<code>情感挖掘</code>，并引入<strong>词袋</strong>概念用于<code>情感标注</code>。</p>
</div></div></div>
<div class="timeline purple"><div class='timeline-item headline'><div class='timeline-item-title'><div class='item-circle'><p><strong>Sentiment Composition</strong></p>
</div></div></div><div class='timeline-item'><div class='timeline-item-title'><div class='item-circle'><p>Valence Shifters, Polanyi &amp; Zaenen 2006</p>
</div></div><div class='timeline-item-content'><p><strong>情感转移器</strong>即可以改变情感取向的单词和短语，这种移位器有几种类型。最常见的类型是<code>否定词</code>。但并不是情感转移器的每一次出现都会改变情感取向，例如，“<strong>不仅……而且</strong>”。</p>
</div></div><div class='timeline-item'><div class='timeline-item-title'><div class='item-circle'><p>Opinion Summarization, Hu &amp; Liu 2004</p>
</div></div><div class='timeline-item-content'><p><strong>观点摘要</strong>是观点挖掘<code>结构化</code>的关键任务，其目标是将一群人的多个观点进行结构化提取和展示，即观点摘要是一群观点的结构化表示。由于结构化需要比较细的粒度，一般用于 <code>Aspect</code> （方面）粒度的摘要。</p>
</div></div></div>
<div class="timeline purple"><div class='timeline-item headline'><div class='timeline-item-title'><div class='item-circle'><p><strong>Lexicons for SA</strong></p>
</div></div></div><div class='timeline-item'><div class='timeline-item-title'><div class='item-circle'><p>Senti/WordNet, ESuli 2006</p>
</div></div><div class='timeline-item-content'><p><strong>Senti/WordNet</strong>是一个有名的情感词典，构筑于Wordnet。 在该词典中，每一个<code>同义词集合</code>依据他们的词义，分别被赋予了肯定、否定和客观的分数。</p>
</div></div><div class='timeline-item'><div class='timeline-item-title'><div class='item-circle'><p>SO-CAL, Taboada 2001</p>
</div></div><div class='timeline-item-content'><p><strong>SO-CAL</strong>是一个基于词汇的<code>情绪计算器</code>。 在其内部的词典中，每个词与它的语义情感相关联(极性和强度)。 它的优势在于能够理解<code>情感转移器</code>，如强化、减弱以及否定。</p>
</div></div></div>
<div class="timeline purple"><div class='timeline-item headline'><div class='timeline-item-title'><div class='item-circle'><p><strong>Sentiment-specific Word Embeddings</strong></p>
</div></div></div><div class='timeline-item'><div class='timeline-item-title'><div class='item-circle'><p>Sentiment Loss, Tang et al 2014</p>
</div></div><div class='timeline-item-content'><p>这项工作提供了为情感分析提供了量身定制的词汇表征，以将<code>情感信息</code>融入了<code>学习损失</code>中来解释情感规律。</p>
</div></div></div>
<div class="timeline purple"><div class='timeline-item headline'><div class='timeline-item-title'><div class='item-circle'><p><strong>Deep Learning</strong></p>
</div></div></div><div class='timeline-item'><div class='timeline-item-title'><div class='item-circle'><p>RNTN, Socher et al 2013</p>
</div></div><div class='timeline-item-content'><p><strong>递归神经张量网络</strong>是深度学习时代的初始模型之一，它可以通过对句子中短语的情感的影响及范围进行建模，来确定句子的情感。</p>
</div></div><div class='timeline-item'><div class='timeline-item-title'><div class='item-circle'><p>CNN, Kim et al 2014</p>
</div></div><div class='timeline-item-content'><p>其提出的<strong>卷积神经网络</strong>具有优越的<code>特征提取</code>和<code>分类能力</code>，因此可以尝试把文本视为一维图像，用CNN来对文本进行分类。</p>
</div></div></div>
<div class="timeline purple"><div class='timeline-item headline'><div class='timeline-item-title'><div class='item-circle'><p><strong>Contextual Language Models</strong></p>
</div></div></div><div class='timeline-item'><div class='timeline-item-title'><div class='item-circle'><p>ULMFit, Howard &amp; Ruder 2018</p>
</div></div><div class='timeline-item-content'><p><strong>ULMFiT</strong>用于实现像CV领域的迁移学习，并可以用于任意NLP任务，其共分为3个阶段，首先是语言模型的预训练、然后是语言模型的finetune、最后是分类任务的finetune</p>
<p><code>finetune</code>即<strong>微调</strong>将冻结预训练模型的部分层（通常是靠近输入的多数层），训练剩下的层（通常是靠近输出的部分层）。</p>
<p>在语言模型的finetune中，作者根据经验对不同层使用了<code>不同的学习率</code>，以提高训练的效率，并使用了倾斜的<code>三角学习率</code>，使参数快速收敛到合适区域后再缓慢调整。</p>
<p>在分类任务的finetune中，<code>逐层解冻</code>前面的层，以防止<strong>灾难性遗忘</strong>（丢失预训练中学习到的内容），并单独训练<code>两个方向</code>的语言模型，最后的预测结果为两个模型的融合。</p>
</div></div><div class='timeline-item'><div class='timeline-item-title'><div class='item-circle'><p>BERT, Devlin 2019</p>
</div></div><div class='timeline-item-content'><p><strong>BERT</strong>是一个预训练的语言表征模型。它采用新的<strong>masked language model（MLM）</strong>，以致能生成<strong>深度的双向</strong>语言表征。其在11种不同NLP测试中创出SOTA表现，是NLP发展史上里程碑式的模型成就。</p>
<p>Masked LM</p>
<ol>
<li>在在每一个训练序列中以15%的概率随机地选中某个token位置用于预测
<ol>
<li>80%的概率下，用<code>[MASK]</code>标记替换该token</li>
<li>10%的概率下，用<code>随机单词</code>替换该token</li>
<li>10%的概率下，该token<code>保持不变</code></li>
</ol>
</li>
<li>预测出原来的token并计算loss</li>
<li>利用这种高度不确定的情况，可以倒逼模型快速学习该token的<code>上下文语义</code>，令到BERT不再只对[MASK]敏感，而是对所有的token都敏感，以致能抽取出任何token的表征信息。</li>
</ol>
</div></div></div>
<h4 id="现状">现状</h4>
<p>如今的NLP领域已经被<code>预训练模型</code>占领了，目前大多情感分析任务也都是在预训练模型的基础上进行的。下图为在IMDB、SST-2/5、Semeval数据集上不同方法的准确率。SOTA（表现最好的模型）无一例外都是基于预训练模型的。<img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://mewtiger-1311904225.cos.ap-nanjing.myqcloud.com/post/image-20220624190047278.png" alt="image-20220624190047278"></p>
<p>由此可以观察到，每当NLP领域出现一种范式转换后，会先将该范式应用于情感分析，再根据情感分析的特点去改善这种模式。将<strong>情感本身特点融入到语言模型中</strong>也正是目前的一个研究热点，最近的很多工作也确实在朝着这个方向努力。</p>
<h3 id="难点与方向">难点与方向</h3>
<h4 id="细粒度的情感分析（ABSA）">细粒度的情感分析（ABSA）</h4>
<p>粗粒度的情感分析越来越不能满足人们对模型可解释性的需求，而<strong>ABSA</strong>几乎满足了大部分场景对情感分析的需要，这种<code>细粒度分析</code>、<code>信息结构化抽取</code>可以说是智能系统必要的能力。下面列举了两个ABSA情感分析主要难点和方向：</p>
<h5 id="1）Implicit-Aspect-Level-Sentiment-Analysis">1）Implicit Aspect-Level Sentiment Analysis</h5>
<p>除了显式的情感之外，还存在<strong>隐式情感</strong>。在如下例子中“<u>这群笨蛋网民竟然不同意的他的观点</u>”。可以明显地观察到，对于网民的情感是消极的。 与此同时，可以推断出，对于观点的情感是积极的。</p>
<p>隐式情感是自然语言中很重要的一部分，理应被重视，但目前大部分对于情感分析的研究多用于显式的文本情感分类问题，采用含有明显情感词的数据集，而对于某些<u>隐式情感的检测和分类效果不佳</u>。</p>
<h6 id="难点：">难点：</h6>
<p>隐式情感的分析</p>
<h5 id="2）Aspect-Term-Polarity-Co-Extraction">2）Aspect Term-Polarity Co-Extraction</h5>
<p>ABSA中现有的大多数算法都认为<strong>方面抽取</strong>和方面级情感分析是顺序或<code>独立</code>的任务，因此都把这些任务当作是独立的子任务来顺序的的解决，而忽略了任务之间的<code>内在联系</code>。</p>
<h6 id="难点：-2">难点：</h6>
<p>对子任务间内在联系的忽略</p>
<h6 id="方向：">方向：</h6>
<p><strong>多任务学习</strong>与<strong>迁移学习</strong></p>
<h5 id="3）Exploiting-Inter-Aspect-Relations-for-Aspect-LevelSentiment-Analysis">3）Exploiting  Inter-Aspect  Relations  for  Aspect-LevelSentiment Analysis</h5>
<p>之前ABSA一般假设<strong>目标之间相互独立</strong>，但在很多情况下目标是<code>相互联系</code>的，常见情况有以下两种</p>
<p><strong>① aspect 并没有任何相关情感词</strong>，需要利用相邻的目标情感来<code>推断</code>，例如“<u>这手机性价比太高了，内存是 256G 的</u>”，其中<strong>手机性价比</strong>有明确的情感和相应情感词，但<strong>手机内存</strong>只有事实说明，没有情感词，需要通过手机性价比来推断该手机内存的情感也是正面的。</p>
<p><strong>②有的 aspects 用连词连接</strong>，两者<code>共用</code>一个情感词，例如“<u>亮度和对比度都还不错</u>”。</p>
<h6 id="难点：-3">难点：</h6>
<p>对目标之间的内在联系的忽略</p>
<h6 id="方向：-2">方向：</h6>
<p>①对<strong>目标</strong>与其情感相关<strong>上下文</strong>间的关系进行建模**[12]**</p>
<p>②对<strong>目标间关系</strong>进行建模**[13]**</p>
<h4 id="上下文信息与其他知识表示">上下文信息与其他知识表示</h4>
<p>利用<strong>上下文信息</strong>（context）和<strong>其他知识表示</strong>一直是 NLP 的技术方向，很多任务单纯从字、词、n元语法模型等文字特征上很难有效学习到深层次的知识。</p>
<h5 id="1）-情感词典">1） 情感词典</h5>
<p>在深度学习之前，往往使用<strong>情感词典</strong>来解决情感分析问题。但单独使用情感词典会遇到许多问题，如词语组合后导致情感取向的变化、同一个词语在不同语境下的多种情感等。但如果将情感词作为一个特征，往往可以为模型提供词<code>粒度的先验知识</code>，提升模型对情感的表示能力。</p>
<h6 id="方向：-3">方向：</h6>
<p>①将词典信息<code>融入</code>到中文NER的CNN结构模型中以加速模型的训练（LexiconRNN）<strong>[14]</strong></p>
<p>②使用<code>注意力机制</code>计算句子中每个词分别和正向、负向词的关系，充分考虑上下文的影响。<strong>[15]</strong></p>
<p>③将 SentiWordNet词典中的情感极性和 POS 信息放入<code>预训练模型</code>中，并在目标函数中引入对 mask 词的极性预测 <strong>[16]</strong></p>
<h5 id="2）-句法知识">2） 句法知识</h5>
<p>将模型中引入<strong>句法知识</strong>是很多 NLP 任务的通用做法，尤其是那些需要对句子成分进行识别或推理的任务。引入句法成分信息有利于模型学习<code>细粒度的特征结构和关系</code>，方面情感分析就是这一类典型场景，其中一个关键子任务是找到 <strong>方面目标</strong>与<strong>情感目标</strong>的关联关系。</p>
<h6 id="方向：-4">方向：</h6>
<p>①使用<strong>依存关系</strong>来做 Aspect 极性分类 <strong>[17]</strong></p>
<p>②将依存和成分信息引入<strong>多任务模型</strong>，然后将<strong>方面抽取</strong>和<strong>极性分类</strong>都当做<strong>序列标注任务</strong> <strong>[18]</strong></p>
<p>③有使用<strong>图卷积网络</strong>来建模<strong>依赖图</strong>的工作 <strong>[19]</strong></p>
<p>④除了句法成分和关系，一些工作还利用了其他<strong>语言学信息</strong>，例如 MEAN 利用<u>情感词典、否定词、程度词注意力机制</u>来做情感分类 <strong>[20]</strong></p>
<h5 id="3）-其他">3） 其他</h5>
<p>①Commonsense Knowledge：<strong>常识</strong>也是大多 NLP 任务想引入的外部知识来源，也一直是热点方向。除了利用外部通用知识（WordNet、ConceptNet 等），也有利用构建好的情感知识库，有很多工作是在 SenticNet 上进行扩展 <strong>[21]</strong></p>
<p>②社交平台上 HashTags：网民在 Twitter、Reddit 等社交平台上使用 <strong>hashtags</strong>、<strong>emoji</strong> 的频率非常高，很多工作将其作为弱监督信号来建模，例如使用 hashtags <strong>[22]</strong>、用 emoji 共现 <strong>[23]</strong> 来做情绪分析。</p>
<h4 id="将情感信息融入预训练模型">将情感信息融入预训练模型</h4>
<p>让预训练模型在<u>预训练阶段就能学习到情感相关的知识</u>，是目前情感分析的趋势。即在预训练自监督模式设计上，加入领域内的知识或假设，构建适合一类任务的<code>专用预训练模型</code>，这也是目前很多<u>提升预训练模型在下游任务表现</u>的有效方法之一。从上一小节的分析，能当做<code>自监督信号</code>大多是情感词典、句法知识、外部知识库、已有情感数据集等，如何将这些知识融入自监督预训练模式是目前很多工作的重点，例如：</p>
<h6 id="1）-SentiBERT-24">1） SentiBERT [24]</h6>
<p>认为情感语义的组合复杂性目前很多模型都没有很好的解决，希望在<strong>预训练</strong>中加入<code>情感的成分信息</code>引导模型学习更多的情感组合。该工作在已有的<strong>情感语义结构数据集</strong>进行再次预训练，利用其中的<code>句法成分</code>和<code>短语极性</code>来做预训练信号的补充。</p>
<ul>
<li>
<h6 id="新增监督信号：">新增监督信号：</h6>
</li>
<li>
<p>成分句法结构中的短语节点的<strong>情感极性</strong></p>
</li>
<li>
<h6 id="预训练数据集：">预训练数据集：</h6>
</li>
<li>
<p>SST-5</p>
</li>
</ul>
<h5 id="2）-SentiLARE-25">2） SentiLARE [25]</h5>
<p>将语言知识加入预训练有两个挑战，一是<u>外部语言知识需要和文本上下文一致</u>，例如情感词典的词极性可能和待训练文本的词义一致。二是<u>外部知识需要能辅助下游任务的目标</u>，例如词典知识需要辅助模型对句子极性的识别。该工作就是考虑如何将外部词典的词极性融入到预训练阶段，为了准确找到文本中的词极性，作者使用大型公开情感词典 <strong>SentiWordNet</strong>，<u>利用词典中的词义的解释和文本进行匹配，找到最合适的词典词义及其词极性</u>。在预训练阶段新增句子极性、mask token 的词性和情感极性的自监督信号。</p>
<ul>
<li>
<h6 id="新增监督信号：-2">新增监督信号：</h6>
</li>
<li>
<p>① 句子情感极性</p>
</li>
<li>
<p>② mask token 的词性</p>
</li>
<li>
<p>③ 情感极性</p>
</li>
<li>
<h6 id="预训练数据集：-2">预训练数据集：</h6>
</li>
<li>
<p>Yelp Dataset Challenge 2019</p>
</li>
<li>
<h6 id="外部知识库：">外部知识库：</h6>
</li>
<li>
<p>SentiWordNet</p>
</li>
</ul>
<h5 id="3）-SKEP-26">3） SKEP [26]</h5>
<p>大多数情感分析都依赖的是<u>情感词、词极性，以及对象和情感词的对应关系</u>，在粗粒度和细粒度情感分析任务中，这几个都是关键学习对象。该工作利用预设的<code>情感词种子</code>和对象的共现程度，在大规模亚马逊评论数据集中挖掘大量的 <code>aspect-opinion pairs</code>，将其作为自监督信号进行预训练。</p>
<ul>
<li>
<h6 id="新增监督信号：-3">新增监督信号：</h6>
</li>
<li>
<p>① mask token 词性</p>
</li>
<li>
<p>② mask 情感词</p>
</li>
<li>
<p>③ 对象-情感词 pair</p>
</li>
<li>
<h6 id="预训练数据集：-3">预训练数据集：</h6>
</li>
<li>
<p>Amazon-2</p>
</li>
</ul>
<h4 id="领域依赖">领域依赖</h4>
<p>是指文本情感分析的模型对某一<strong>领域</strong>的文本数据非常<strong>有效</strong>，但是将其应用于其他领域的时候，会使得分类模型的<strong>性能严重下降</strong>。这也是 NLP 的通用问题，在业界应用中广泛存在，在这个方向上突破将有很大的实际意义。</p>
<h5 id="1）-Multi-Domain">1） Multi-Domain</h5>
<p><strong>Multi-Domain</strong> 指的是任务作用域有<code>多种场景</code>，场景之间不论从特征空间还是语义表示都可能存在较大差异。同一个词可能在不同场景<code>情感极性差异</code>会很大。如果只针对某个领域进行优化，虽然能提升单一场景的效果，但可能会造成对该场景的<strong>过拟合</strong>，使之在其他场景中性能不佳。</p>
<h5 id="难点：-4">难点：</h5>
<p><u>既保留场景间的共性，又能表示场景间的个性</u></p>
<h5 id="方向">方向:</h5>
<h6 id="①-Shared-Private-network-27">① Shared-Private network[27]</h6>
<p>用一些 networks 来学习领域无关（domain-agnostic）即各领域<strong>共享的特征</strong>，用另一些 networks 学习领域相关（domain-specific）即<strong>单个领域的特征</strong>。该文章提出了一个多任务框架，多种任务之间可以通过一个<code>外部memory</code>共享信息。</p>
<h6 id="②-Adversarial-multi-task-learning-28">② Adversarial multi-task learning[28]</h6>
<p>多任务学习的重点是学习<code>共享层</code>，其必须包含更多的共同特征和不包含特定任务的特征。该<strong>多任务对抗网络</strong>为此作了如下工作</p>
<ol>
<li>以精确的方式划分了任务<code>特定（私有）空间</code>和<code>共享空间</code></li>
<li>使用<code>对抗训练</code>用于确保共享空间只包含共有的信息</li>
<li>使用<code>正交约束</code>用来消除共享和特有空间冗余的特征</li>
</ol>
<h6 id="③-Private-to-shared-attention-mechanism-29">③ Private to shared attention mechanism[29]</h6>
<p>利用<code>自注意力</code>机制在<strong>共享特征</strong>中挑选合适的<strong>私有特征</strong>。</p>
<h5 id="2）-Cross-Domain">2） Cross-Domain</h5>
<p>跨领域/领域迁移，旨在解决目标领域没有或只有少量的标注语料的问题，可以利用的前提条件是在另一个相似领域有足够多的语料，即<code>迁移学习</code>。</p>
<h5 id="难点：-5">难点：</h5>
<p><u>语义在不同领域不一致</u></p>
<h5 id="方向：-5">方向：</h5>
<h6 id="①-前预训练时期">① 前预训练时期</h6>
<p>在预训练统治期之前，解决跨领域的方式跟多领域有相似的地方。</p>
<p><strong>Domain Adapted Word Embeddings</strong> 通过分别捕获<strong>特定语义</strong>与<strong>非特定语义</strong>，并将其组合，来解决了词向量在<strong>通用性</strong>与<strong>特定性</strong>之间的矛盾**[30]**。</p>
<p><strong>可迁移信息识别</strong>能自动提取在跨域中具有相同重要度和<code>一致极性</code>的词，作为用于跨域情感分析的可用信息 <strong>[31]</strong>。</p>
<h6 id="②-Finetune">② Finetune</h6>
<p>通过源域数据训练base模型，将base模型使用目标域数据经过多个阶段微调</p>
<h6 id="③-Selective-masking-32">③ Selective masking[32]</h6>
<p>传统预训练模型在 pretrain 阶段进行的 <strong>token mask</strong> 时太随意，会导致学习到的通用表示无法很好适用于下游任务。</p>
<p>使用 <strong>selective masking</strong> 策略，挑选对下游任务有<code>较大影响</code>的 tokens 进行 mask，增加 pretrain 阶段的目的性，使得到的预训练模型再作用于下游任务将会有更好的<code>任务特定表征</code>。</p>
<h6 id="④-DomBERT-33">④ DomBERT[33]</h6>
<p>迁移学习在源领域与目标领域有<code>较强相关性</code>的时候才有较好的效果，因此该方法在很多的源领域中自动选择和目标领域<strong>相似</strong>的训练样本来进行<strong>预训练</strong>。</p>
<p><strong>DomBERT</strong> 通过用大量不同领域的非标注语料来训练<code>领域分类器</code>，利用该分类器来计算不同<strong>领域样本</strong>的<code>相似度</code>，然后用相似度归一化作为选择的概率，这样就可以在每次 mini-batch 训练时，自动选择相似度最高的样本。</p>
<h2 id="参考资料">参考资料</h2>
<p><strong>[1]</strong> 王婷,杨文忠.文本情感分析方法研究综述[J].计算机工程与应用,2021,57(12):11-24.</p>
<p><strong>[2]</strong> 洪巍,李敏.文本情感分析方法研究综述[J].计算机工程与科学,2019,41(04):750-757.</p>
<p><strong>[3]</strong> 全面解读文本情感分析, <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/270399396">https://zhuanlan.zhihu.com/p/270399396</a></p>
<p><strong>[4]</strong> 华为云细粒度文本情感分析及应用 , <a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_27590277/article/details/114465660">https://blog.csdn.net/qq_27590277/article/details/114465660</a></p>
<p><strong>[5]</strong> 一文看懂情感分析技术应用与趋势 , <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/354306620">https://zhuanlan.zhihu.com/p/354306620</a></p>
<p><strong>[6]</strong> 情感分析的基础知识介绍 , <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/391712395">https://zhuanlan.zhihu.com/p/391712395</a></p>
<p><strong>[7]</strong> 文本情感分析方法研究小结, <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/106588589">https://zhuanlan.zhihu.com/p/106588589</a></p>
<p><strong>[8]</strong> Sentiment Analysis of a Tweet With Naive Bayes, <a target="_blank" rel="noopener" href="https://towardsdatascience.com/sentiment-analysis-of-a-tweet-with-naive-bayes-ff9bdb2949c7">https://towardsdatascience.com/sentiment-analysis-of-a-tweet-with-naive-bayes-ff9bdb2949c7</a></p>
<p><strong>[9]</strong> Poria S ,  Hazarika D ,  Majumder N , et al. Beneath the Tip of the Iceberg: Current Challenges and New Directions in Sentiment Analysis Research[J].  2020.</p>
<p><strong>[10]</strong> 什么是BERT？, <a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/98855346">https://zhuanlan.zhihu.com/p/98855346</a></p>
<p><strong>[11]</strong> Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. 2016. Deep multi-task learning with shared memory. In Proceedings of EMNLP.</p>
<p><strong>[12]</strong> Tang, D., Qin, B., Feng, X., and Liu, T.  Effective lstms fortarget-dependent sentiment classification. InCOLING 2016,26th International Conference on Computational Linguistics,Proceedings of the Conference: Technical Papers, December 11-16, 2016, Osaka, Japan, pp. 3298–3307. ACL, 2016a</p>
<p><strong>[13]</strong> Hazarika, D., Poria, S., Vij, P., Krishnamurthy, G., Cambria, E.,and Zimmermann, R.  Modeling inter-aspect dependenciesfor  aspect-based  sentiment  analysis.    InProceedings  ofthe  2018  Conference  of  the  North  American  Chapter  of  theAssociation for Computational Linguistics: Human LanguageTechnologies,  NAACL-HLT,  New  Orleans,  Louisiana,  USA,June 1-6, 2018, Volume 2 (Short Papers), pp. 266–270. Associ-ation for Computational Linguistics, 2018c.</p>
<p><strong>[14]</strong> Zhiyang Teng, Duy-Tin Vo, and Yue Zhang. 2016. Context-sensitive lexicon features for neural sentiment analysis. In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages 1629–1638.</p>
<p><strong>[15]</strong> Yi Tay, Anh Tuan Luu, Siu Cheung Hui, and Jian Su. 2018. Attentive Gated Lexicon Reader with Contrastive Contextual Co-attention for Sentiment Classiﬁcation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3443–3453.</p>
<p><strong>[16]</strong> Andrea Esuli and Fabrizio Sebastiani. 2006. SENTIWORDNET: A publicly available lexical resource for opinion mining. In Proceedings of the Fifth International Conference on Language Resources and Evaluation, pages 417–422.</p>
<p><strong>[17]</strong> Xuefeng Bai, Pengbo Liu, Y. Zhang, 2020. Exploiting Typed Syntactic Dependencies for Targeted Sentiment Classiﬁcation Using Graph Attention Neural Network.</p>
<p><strong>[18]</strong> Yunlong Liang, Fandong Meng, Jinchao Zhang, Jinan Xu, Yufeng Chen, and Jie Zhou. 2020a. A dependency syntactic knowledge augmented interactive architecture for end-to-end aspect-based sentiment analysis. arXiv preprint arXiv:2004.01951.</p>
<p><strong>[19]</strong> Chen Zhang, Qiuchi Li, and Dawei Song. 2019. Aspect-based sentiment classiﬁcation with aspectspeciﬁc graph convolutional networks. arXiv preprint arXiv:1909.03477.</p>
<p><strong>[20]</strong> Zeyang Lei, Yujiu Yang, Min Yang, and Yi Liu. 2018. A multi-sentiment-resource enhanced attention network for sentiment classiﬁcation. In ACL 2018.</p>
<p><strong>[21]</strong> Cambria E, Olsher D, Rajagopal D. SenticNet 3: a common and common-sense knowledge base for cognition-driven sentiment analysis. Quebec City: AAAI; 2014. p. 1515–21.</p>
<p><strong>[22]</strong> FA Kunneman, CC Liebrecht, and APJ van den Bosch. 2014. The (un)predictability of emotional hashtags in twitter. In 52th Annual Meeting of the Association for Computational Linguistics (ACL). Association for Computational Linguistics.</p>
<p><strong>[23]</strong> Felbo B, Mislove A, Søgaard A, Rahwan I, Lehmann S. Using millions of Emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm; 2017. arXiv preprint arXiv:1708.00524.</p>
<p><strong>[24]</strong> Da Yin, Tao Meng, and Kai-Wei Chang. 2020. Sentibert: A transferable transformer-based architecture for compositional sentiment semantics. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3695–3706.</p>
<p><strong>[25]</strong> Pei Ke, Haozhe Ji, Siyang Liu, Xiaoyan Zhu, and Minlie Huang. 2020. SentiLARE: Sentiment-aware language representation learning with linguistic knowledge. In Proceedings of EMNLP.</p>
<p><strong>[26]</strong> Hao Tian, Can Gao, Xinyan Xiao, Hao Liu, Bolei He, Hua Wu, Haifeng Wang, and Feng Wu. 2020. SKEP: Sentiment knowledge enhanced pre-training for sentiment analysis. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4067–4076, Online. Association for Computational Linguistics.</p>
<p><strong>[27]</strong> Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. 2016. Deep multi-task learning with shared memory. In Proceedings of EMNLP.</p>
<p><strong>[28]</strong> Pengfei Liu, Xipeng Qiu, and Xuanjing Huang. 2017. Adversarial multi-task learning for text classiﬁcation. In ACL, pages 1–10.</p>
<p><strong>[29]</strong> Renjie Zheng, Junkun Chen, and Xipeng Qiu. 2018. Same Representation, Different Attentions: Shareable Sentence Representation Learning from Multiple Tasks. international joint conference on artificial intelligence, pages 4616–4622.</p>
<p><strong>[30]</strong> Sarma, P. K., Liang, Y., and Sethares, B. Domain adapted word embeddings for improved sentiment classiﬁcation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 2: Short Papers, pp. 37–42, 2018.</p>
<p><strong>[31]</strong> Sharma, R., Bhattacharyya, P., Dandapat, S., and Bhatt, H. S. Identifying transferable information across domains for cross-domain sentiment classiﬁcation. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 1: Long Papers, pp. 968–978, 2018.</p>
<p><strong>[32]</strong> Yuxian Gu, Zhengyan Zhang, Xiaozhi Wang, Zhiyuan Liu, and Maosong Sun. Train no evil: Selective masking for task-guided pre-training. arXiv preprint arXiv:2004.09733, 2020.</p>
<p><strong>[33]</strong> Hu Xu, Bing Liu, Lei Shu, and Philip S. Yu. 2020. Dombert: Domain-oriented language model for aspect-based sentiment analysis. arXiv preprint arXiv: 2004.13816.</p>
<p><strong>[34]</strong> Zhang W ,  Li X ,  Deng Y , et al. A Survey on Aspect-Based Sentiment Analysis: Tasks, Methods, and Challenges[J].  2022.</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a target="_blank" rel="noopener" href="https://mewtiger.top">MewTiger</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://81.69.237.168/2022/06/23/%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90%E8%B0%83%E7%A0%94/">http://81.69.237.168/2022/06/23/情感分析调研/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://81.69.237.168" target="_blank">喵喵虎冲呀</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E5%A4%A7%E4%B8%89%E4%B8%8B/">大三下</a><a class="post-meta__tags" href="/tags/%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90/">情感分析</a><a class="post-meta__tags" href="/tags/%E8%B0%83%E7%A0%94/">调研</a></div><div class="post_share"></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2022/06/19/%E8%AE%AD%E7%BB%83%E5%AE%89%E6%8E%92/"><img class="next-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/1.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">假期减脂训练计划</div></div></a></div></nav><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://mewtiger-1311904225.cos.ap-nanjing.myqcloud.com/post/7A4D4AF18D661DBAB5D69C3BE2A3F8F4.png" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">MewTiger</div><div class="author-info__description">In me the tiger sniffs the rose</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">72</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">31</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">8</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/MananaFX"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/MananaFX" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:nbspfjh@126.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="https://xn--5ts203a3u7b.xn--6qq986b3xl" target="_blank" title="Manana"><i class="iconfont icon-rabbit1"></i></a><a class="social-icon" href="https://www.douban.com/people/169659815/?_i=39037888bisBXk" target="_blank" title="douban"><i class="iconfont icon-douban"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90%E5%9F%BA%E7%A1%80"><span class="toc-number">1.</span> <span class="toc-text">情感分析基础</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A6%82%E5%BF%B5"><span class="toc-number">1.1.</span> <span class="toc-text">概念</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%B2%92%E5%BA%A6"><span class="toc-number">1.2.</span> <span class="toc-text">粒度</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AF%8D%E7%BA%A7%E5%88%AB"><span class="toc-number">1.2.1.</span> <span class="toc-text">词级别</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8F%A5%E5%AD%90-%E6%96%87%E6%A1%A3%E7%BA%A7"><span class="toc-number">1.2.2.</span> <span class="toc-text">句子&#x2F;文档级</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%9B%AE%E6%A0%87%E7%BA%A7"><span class="toc-number">1.2.3.</span> <span class="toc-text">目标级</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1%EF%BC%89-Target-no-aspect-based-sentiment-analysis-TN-ABSA"><span class="toc-number">1.2.3.1.</span> <span class="toc-text">1） Target no aspect based sentiment analysis (TN-ABSA)</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2%EF%BC%89-Target-grounded-aspect-based-sentiment-analysis-TG-ABSA"><span class="toc-number">1.2.3.2.</span> <span class="toc-text">2） Target-grounded aspect based sentiment analysis (TG-ABSA)</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3%EF%BC%89-Target-aspect-based-sentiment-analysis-T-ABSA"><span class="toc-number">1.2.3.3.</span> <span class="toc-text">3） Target aspect based sentiment analysis (T-ABSA)</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90%E8%A6%81%E7%B4%A0"><span class="toc-number">1.3.</span> <span class="toc-text">情感分析要素</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90%E4%BA%94%E5%85%83%E7%B4%A0"><span class="toc-number">1.3.1.</span> <span class="toc-text">情感分析五元素</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AF%B9%E8%B1%A1%E7%BB%93%E6%9E%84%E5%8C%96%E8%A1%A8%E7%A4%BA"><span class="toc-number">1.3.2.</span> <span class="toc-text">对象结构化表示</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%83%85%E6%84%9F%E7%BB%93%E6%9E%84%E5%8C%96%E8%A1%A8%E7%A4%BA"><span class="toc-number">1.3.3.</span> <span class="toc-text">情感结构化表示</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%9C%E7%94%A8-2"><span class="toc-number">1.4.</span> <span class="toc-text">作用</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%96%B9%E6%B3%95"><span class="toc-number">2.</span> <span class="toc-text">方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%83%85%E6%84%9F%E8%AF%8D%E5%85%B8%E6%96%B9%E6%B3%95"><span class="toc-number">2.1.</span> <span class="toc-text">情感词典方法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BA%BA%E5%B7%A5%E6%9E%84%E5%BB%BA%E6%83%85%E6%84%9F%E5%AD%97%E5%85%B8"><span class="toc-number">2.1.1.</span> <span class="toc-text">人工构建情感字典</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%87%AA%E5%8A%A8%E6%9E%84%E5%BB%BA%E6%83%85%E6%84%9F%E5%AD%97%E5%85%B8"><span class="toc-number">2.1.2.</span> <span class="toc-text">自动构建情感字典</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B0%8F%E7%BB%93-2"><span class="toc-number">2.1.3.</span> <span class="toc-text">小结</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E4%BC%A0%E7%BB%9F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90%E6%96%B9%E6%B3%95"><span class="toc-number">2.2.</span> <span class="toc-text">基于传统机器学习的情感分析方法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF"><span class="toc-number">2.2.1.</span> <span class="toc-text">朴素贝叶斯</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1%EF%BC%89-%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%85%AC%E5%BC%8F"><span class="toc-number">2.2.1.1.</span> <span class="toc-text">1） 贝叶斯公式</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#%E4%BE%8B%E5%AD%90"><span class="toc-number">2.2.1.1.1.</span> <span class="toc-text">例子</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2%EF%BC%89-%E6%8B%89%E6%99%AE%E6%8B%89%E6%96%AF%E5%B9%B3%E6%BB%91-Laplace-Smoothing"><span class="toc-number">2.2.1.2.</span> <span class="toc-text">2） 拉普拉斯平滑 (Laplace Smoothing)</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3%EF%BC%89-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF"><span class="toc-number">2.2.1.3.</span> <span class="toc-text">3） 朴素贝叶斯</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#4%EF%BC%89-%E7%BC%BA%E7%82%B9"><span class="toc-number">2.2.1.4.</span> <span class="toc-text">4） 缺点</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%B4%E8%BF%91%E7%AE%97%E6%B3%95%EF%BC%88KNN"><span class="toc-number">2.2.2.</span> <span class="toc-text">临近算法（KNN)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88SVM%EF%BC%89"><span class="toc-number">2.2.3.</span> <span class="toc-text">支持向量机（SVM）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B0%8F%E7%BB%93-3"><span class="toc-number">2.2.4.</span> <span class="toc-text">小结</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90%E6%96%B9%E6%B3%95"><span class="toc-number">2.3.</span> <span class="toc-text">基于深度学习的情感分析方法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8D%95%E4%B8%80%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90%E6%96%B9%E6%B3%95"><span class="toc-number">2.3.1.</span> <span class="toc-text">单一神经网络的情感分析方法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%B7%B7%E5%90%88%EF%BC%88%E7%BB%84%E5%90%88%E3%80%81%E8%9E%8D%E5%90%88%EF%BC%89%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90%E6%96%B9%E6%B3%95"><span class="toc-number">2.3.2.</span> <span class="toc-text">混合（组合、融合）神经网络的情感分析方法</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BC%95%E5%85%A5%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E7%9A%84%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90"><span class="toc-number">2.3.3.</span> <span class="toc-text">引入注意力机制的情感分析</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90"><span class="toc-number">2.3.4.</span> <span class="toc-text">使用预训练模型的情感分析</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B0%8F%E7%BB%93-4"><span class="toc-number">2.3.5.</span> <span class="toc-text">小结</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%9B%B8%E5%85%B3%E4%BB%BB%E5%8A%A1"><span class="toc-number">3.</span> <span class="toc-text">相关任务</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%96%B9%E9%9D%A2%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90%EF%BC%88ABSA%EF%BC%89"><span class="toc-number">3.1.</span> <span class="toc-text">方面情感分析（ABSA）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%83%85%E7%BB%AA%E8%AF%86%E5%88%AB%EF%BC%88Emotion-Detection%EF%BC%89"><span class="toc-number">3.2.</span> <span class="toc-text">情绪识别（Emotion Detection）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AF%B9%E8%AF%9D%E7%9B%B8%E5%85%B3"><span class="toc-number">3.3.</span> <span class="toc-text">对话相关</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AF%B9%E8%AF%9D%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90"><span class="toc-number">3.3.1.</span> <span class="toc-text">对话情感分析</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%83%85%E6%84%9F%E5%AF%B9%E8%AF%9D%E7%94%9F%E6%88%90"><span class="toc-number">3.3.2.</span> <span class="toc-text">情感对话生成</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A7%82%E7%82%B9%E6%91%98%E8%A6%81"><span class="toc-number">3.4.</span> <span class="toc-text">观点摘要</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AB%8B%E5%9C%BA%E8%AF%86%E5%88%AB%EF%BC%88Stance-Detection%EF%BC%89"><span class="toc-number">3.5.</span> <span class="toc-text">立场识别（Stance Detection）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%8D%E8%AE%BD%E8%AF%86%E5%88%AB"><span class="toc-number">3.6.</span> <span class="toc-text">反讽识别</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%83%85%E6%84%9F%E5%8E%9F%E5%9B%A0%E6%8E%A8%E7%90%86"><span class="toc-number">3.7.</span> <span class="toc-text">情感原因推理</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%8E%B0%E7%8A%B6%E3%80%81%E9%97%AE%E9%A2%98%E5%92%8C%E6%96%B9%E5%90%91"><span class="toc-number">4.</span> <span class="toc-text">现状、问题和方向</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%91%E5%B1%95%E4%B8%8E%E7%8E%B0%E7%8A%B6"><span class="toc-number">4.1.</span> <span class="toc-text">发展与现状</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8F%91%E5%B1%95"><span class="toc-number">4.1.1.</span> <span class="toc-text">发展</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%8E%B0%E7%8A%B6"><span class="toc-number">4.1.2.</span> <span class="toc-text">现状</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9A%BE%E7%82%B9%E4%B8%8E%E6%96%B9%E5%90%91"><span class="toc-number">4.2.</span> <span class="toc-text">难点与方向</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BB%86%E7%B2%92%E5%BA%A6%E7%9A%84%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90%EF%BC%88ABSA%EF%BC%89"><span class="toc-number">4.2.1.</span> <span class="toc-text">细粒度的情感分析（ABSA）</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1%EF%BC%89Implicit-Aspect-Level-Sentiment-Analysis"><span class="toc-number">4.2.1.1.</span> <span class="toc-text">1）Implicit Aspect-Level Sentiment Analysis</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#%E9%9A%BE%E7%82%B9%EF%BC%9A"><span class="toc-number">4.2.1.1.1.</span> <span class="toc-text">难点：</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2%EF%BC%89Aspect-Term-Polarity-Co-Extraction"><span class="toc-number">4.2.1.2.</span> <span class="toc-text">2）Aspect Term-Polarity Co-Extraction</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#%E9%9A%BE%E7%82%B9%EF%BC%9A-2"><span class="toc-number">4.2.1.2.1.</span> <span class="toc-text">难点：</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%E6%96%B9%E5%90%91%EF%BC%9A"><span class="toc-number">4.2.1.2.2.</span> <span class="toc-text">方向：</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3%EF%BC%89Exploiting-Inter-Aspect-Relations-for-Aspect-LevelSentiment-Analysis"><span class="toc-number">4.2.1.3.</span> <span class="toc-text">3）Exploiting  Inter-Aspect  Relations  for  Aspect-LevelSentiment Analysis</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#%E9%9A%BE%E7%82%B9%EF%BC%9A-3"><span class="toc-number">4.2.1.3.1.</span> <span class="toc-text">难点：</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%E6%96%B9%E5%90%91%EF%BC%9A-2"><span class="toc-number">4.2.1.3.2.</span> <span class="toc-text">方向：</span></a></li></ol></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%8A%E4%B8%8B%E6%96%87%E4%BF%A1%E6%81%AF%E4%B8%8E%E5%85%B6%E4%BB%96%E7%9F%A5%E8%AF%86%E8%A1%A8%E7%A4%BA"><span class="toc-number">4.2.2.</span> <span class="toc-text">上下文信息与其他知识表示</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1%EF%BC%89-%E6%83%85%E6%84%9F%E8%AF%8D%E5%85%B8"><span class="toc-number">4.2.2.1.</span> <span class="toc-text">1） 情感词典</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#%E6%96%B9%E5%90%91%EF%BC%9A-3"><span class="toc-number">4.2.2.1.1.</span> <span class="toc-text">方向：</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2%EF%BC%89-%E5%8F%A5%E6%B3%95%E7%9F%A5%E8%AF%86"><span class="toc-number">4.2.2.2.</span> <span class="toc-text">2） 句法知识</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#%E6%96%B9%E5%90%91%EF%BC%9A-4"><span class="toc-number">4.2.2.2.1.</span> <span class="toc-text">方向：</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3%EF%BC%89-%E5%85%B6%E4%BB%96"><span class="toc-number">4.2.2.3.</span> <span class="toc-text">3） 其他</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%B0%86%E6%83%85%E6%84%9F%E4%BF%A1%E6%81%AF%E8%9E%8D%E5%85%A5%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B"><span class="toc-number">4.2.3.</span> <span class="toc-text">将情感信息融入预训练模型</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#1%EF%BC%89-SentiBERT-24"><span class="toc-number">4.2.3.0.1.</span> <span class="toc-text">1） SentiBERT [24]</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%E6%96%B0%E5%A2%9E%E7%9B%91%E7%9D%A3%E4%BF%A1%E5%8F%B7%EF%BC%9A"><span class="toc-number">4.2.3.0.2.</span> <span class="toc-text">新增监督信号：</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%E9%A2%84%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E9%9B%86%EF%BC%9A"><span class="toc-number">4.2.3.0.3.</span> <span class="toc-text">预训练数据集：</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2%EF%BC%89-SentiLARE-25"><span class="toc-number">4.2.3.1.</span> <span class="toc-text">2） SentiLARE [25]</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#%E6%96%B0%E5%A2%9E%E7%9B%91%E7%9D%A3%E4%BF%A1%E5%8F%B7%EF%BC%9A-2"><span class="toc-number">4.2.3.1.1.</span> <span class="toc-text">新增监督信号：</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%E9%A2%84%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E9%9B%86%EF%BC%9A-2"><span class="toc-number">4.2.3.1.2.</span> <span class="toc-text">预训练数据集：</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%E5%A4%96%E9%83%A8%E7%9F%A5%E8%AF%86%E5%BA%93%EF%BC%9A"><span class="toc-number">4.2.3.1.3.</span> <span class="toc-text">外部知识库：</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#3%EF%BC%89-SKEP-26"><span class="toc-number">4.2.3.2.</span> <span class="toc-text">3） SKEP [26]</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#%E6%96%B0%E5%A2%9E%E7%9B%91%E7%9D%A3%E4%BF%A1%E5%8F%B7%EF%BC%9A-3"><span class="toc-number">4.2.3.2.1.</span> <span class="toc-text">新增监督信号：</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%E9%A2%84%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE%E9%9B%86%EF%BC%9A-3"><span class="toc-number">4.2.3.2.2.</span> <span class="toc-text">预训练数据集：</span></a></li></ol></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%A2%86%E5%9F%9F%E4%BE%9D%E8%B5%96"><span class="toc-number">4.2.4.</span> <span class="toc-text">领域依赖</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#1%EF%BC%89-Multi-Domain"><span class="toc-number">4.2.4.1.</span> <span class="toc-text">1） Multi-Domain</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E9%9A%BE%E7%82%B9%EF%BC%9A-4"><span class="toc-number">4.2.4.2.</span> <span class="toc-text">难点：</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%96%B9%E5%90%91"><span class="toc-number">4.2.4.3.</span> <span class="toc-text">方向:</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#%E2%91%A0-Shared-Private-network-27"><span class="toc-number">4.2.4.3.1.</span> <span class="toc-text">① Shared-Private network[27]</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%E2%91%A1-Adversarial-multi-task-learning-28"><span class="toc-number">4.2.4.3.2.</span> <span class="toc-text">② Adversarial multi-task learning[28]</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%E2%91%A2-Private-to-shared-attention-mechanism-29"><span class="toc-number">4.2.4.3.3.</span> <span class="toc-text">③ Private to shared attention mechanism[29]</span></a></li></ol></li><li class="toc-item toc-level-5"><a class="toc-link" href="#2%EF%BC%89-Cross-Domain"><span class="toc-number">4.2.4.4.</span> <span class="toc-text">2） Cross-Domain</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E9%9A%BE%E7%82%B9%EF%BC%9A-5"><span class="toc-number">4.2.4.5.</span> <span class="toc-text">难点：</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%96%B9%E5%90%91%EF%BC%9A-5"><span class="toc-number">4.2.4.6.</span> <span class="toc-text">方向：</span></a><ol class="toc-child"><li class="toc-item toc-level-6"><a class="toc-link" href="#%E2%91%A0-%E5%89%8D%E9%A2%84%E8%AE%AD%E7%BB%83%E6%97%B6%E6%9C%9F"><span class="toc-number">4.2.4.6.1.</span> <span class="toc-text">① 前预训练时期</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%E2%91%A1-Finetune"><span class="toc-number">4.2.4.6.2.</span> <span class="toc-text">② Finetune</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%E2%91%A2-Selective-masking-32"><span class="toc-number">4.2.4.6.3.</span> <span class="toc-text">③ Selective masking[32]</span></a></li><li class="toc-item toc-level-6"><a class="toc-link" href="#%E2%91%A3-DomBERT-33"><span class="toc-number">4.2.4.6.4.</span> <span class="toc-text">④ DomBERT[33]</span></a></li></ol></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="toc-number">5.</span> <span class="toc-text">参考资料</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2022/06/23/%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90%E8%B0%83%E7%A0%94/" title="情感分析调研"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/7.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="情感分析调研"/></a><div class="content"><a class="title" href="/2022/06/23/%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90%E8%B0%83%E7%A0%94/" title="情感分析调研">情感分析调研</a><time datetime="2022-06-23T04:35:17.000Z" title="发表于 2022-06-23 12:35:17">2022-06-23</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/06/19/%E8%AE%AD%E7%BB%83%E5%AE%89%E6%8E%92/" title="假期减脂训练计划"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/1.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="假期减脂训练计划"/></a><div class="content"><a class="title" href="/2022/06/19/%E8%AE%AD%E7%BB%83%E5%AE%89%E6%8E%92/" title="假期减脂训练计划">假期减脂训练计划</a><time datetime="2022-06-19T01:54:38.000Z" title="发表于 2022-06-19 09:54:38">2022-06-19</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/06/04/%E6%B9%BF%E6%B8%A9%E7%97%85/" title="湿温病"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://mewtiger-1311904225.cos.ap-nanjing.myqcloud.com/liman/image-20220601161404223.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="湿温病"/></a><div class="content"><a class="title" href="/2022/06/04/%E6%B9%BF%E6%B8%A9%E7%97%85/" title="湿温病">湿温病</a><time datetime="2022-06-04T03:21:51.610Z" title="发表于 2022-06-04 11:21:51">2022-06-04</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/06/04/%E7%B1%BB%E5%88%AB%E5%8F%8A%E6%A0%87%E7%AD%BE%E6%B8%85%E5%8D%95/" title="类别及标签清单"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="/img/11.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="类别及标签清单"/></a><div class="content"><a class="title" href="/2022/06/04/%E7%B1%BB%E5%88%AB%E5%8F%8A%E6%A0%87%E7%AD%BE%E6%B8%85%E5%8D%95/" title="类别及标签清单">类别及标签清单</a><time datetime="2022-06-04T03:09:28.000Z" title="发表于 2022-06-04 11:09:28">2022-06-04</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2022/06/04/6-18%E6%9D%AD%E5%B7%9E%E8%A1%8C/" title="6.18杭州行"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://mewtiger-1311904225.cos.ap-nanjing.myqcloud.com/post/f1d5bb0c2e16f77be9f1ad38285a938.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="6.18杭州行"/></a><div class="content"><a class="title" href="/2022/06/04/6-18%E6%9D%AD%E5%B7%9E%E8%A1%8C/" title="6.18杭州行">6.18杭州行</a><time datetime="2022-06-04T02:42:15.000Z" title="发表于 2022-06-04 10:42:15">2022-06-04</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2021 - 2022 By MewTiger</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text"><a target="_blank" rel="noopener" href="https://beian.miit.gov.cn"><span>浙ICP备2022015449号-2</span></a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="//instant.page/5.1.0" type="module"></script><script src="https://cdn.xn--5ts203a3u7b.xn--6qq986b3xl/vanilla-lazyload-master/dist/lazyload.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>(()=>{
  const init = () => {
    twikoo.init(Object.assign({
      el: '#twikoo-wrap',
      envId: 'cloudbase-prepaid-1es1bia77588a6',
      region: '',
      onCommentLoaded: function () {
        btf.loadLightbox(document.querySelectorAll('#twikoo .tk-content img:not(.tk-owo-emotion)'))
      }
    }, null))
  }

  const getCount = () => {
    const countELement = document.getElementById('twikoo-count')
    if(!countELement) return
    twikoo.getCommentsCount({
      envId: 'cloudbase-prepaid-1es1bia77588a6',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(function (res) {
      countELement.innerText = res[0].count
    }).catch(function (err) {
      console.error(err);
    });
  }

  const runFn = () => {
    init()
    GLOBAL_CONFIG_SITE.isPost && getCount()
  }

  const loadTwikoo = () => {
    if (typeof twikoo === 'object') {
      setTimeout(runFn,0)
      return
    } 
    getScript('https://cdn.staticfile.org/twikoo/1.5.10/twikoo.all.min.js').then(runFn)
  }

  if ('Twikoo' === 'Twikoo' || !true) {
    if (true) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo()
  } else {
    window.loadOtherComment = () => {
      loadTwikoo()
    }
  }
})()</script></div><script async src="https://cdn.xn--5ts203a3u7b.xn--6qq986b3xl/live2dw/autoload.js"></script><script defer="defer" id="ribbon" src="https://cdn.xn--5ts203a3u7b.xn--6qq986b3xl/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="false" data-click="false"></script></div><!-- hexo injector body_end start --><script async src="//at.alicdn.com/t/font_2032782_8d5kxvn09md.js"></script><!-- hexo injector body_end end --></body></html>