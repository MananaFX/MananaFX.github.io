---
title: CS224N
date: 2022-10-23 18:30:54
categories: NLP
tags: [å¤§å››ä¸Š,CS224N,NLP]

---

# Word Vectors

## Word2vec: prediction function



## 1.Skip-gram

å·²çŸ¥å½“å‰è¯è¯­ï¼Œé¢„æµ‹ä¸Šä¸‹æ–‡

### **L(Î¸)**:	<u>date Likelihood</u> of the capacity of predicting words in context 

**m**:  size of predicting window

**w<sub>t</sub>**: given center word 

å¯¹äºæ¯ä¸€ä¸ªä½ç½®tï¼Œç»™å®šçª—å£å†…æ¯ä¸ªè¯w<sub>t+j</sub>é¢„æµ‹å‡ºå®ƒçš„æ¦‚ç‡çš„ç´¯ä¹˜
$$
L(\theta)=âˆ_{t=1}^{T}âˆ_{-mâ‰¤jâ‰¤m,jâ‰ 0}P(w_{t+j}|w_t;\theta)
$$


### **J(Î¸):**  <u>objective function / loss function</u>

$$
J(\theta)=-\frac{1}{T}logL(\theta)=-\frac{1}{T}âˆ‘_{t=1}^{T}âˆ‘_{-mâ‰¤jâ‰¤m,jâ‰ 0}logP(w_{t+j}|w_t;\theta)
$$

ç´¯ä¹˜æ±‚æœ€å¤§å€¼ =ã€‹ç´¯åŠ æ±‚æœ€å°å€¼



### Softmax Functionâ€”â€”<u>Scale</u>

- **max**: amplify  the probability of larger x<sub>i</sub>
- **soft**: still assign some probability of smaller x<sub>i</sub>

$$
softmax(x) = \frac{exp(x)}{âˆ‘^n_{j=1}exp(x_j)}
$$



### Prediction Function

use two vectors per word **w** to simplify <u>math and optimization</u> and can be <u>built easily</u>.

- **v<sub>w</sub>**: vector for **center** word w
- **u<sub>w</sub>**: vector of **context** word w

**c** : center word

**o** : context word
$$
P(o|c) = \frac{exp(u_o^Tv_c)}{âˆ‘_{wâˆˆV}exp(u^T_wv_c)}
$$
**u<sup>T</sup><sub>o</sub>v<sub>c</sub>** : **similarity** of o and c

é™¤æ³•ä¸ºäº†å½’ä¸€åŒ–



### Gradient Descent

$$
\theta^{new} = \theta^{old} - \alpha \Delta_\theta J(\theta)
$$

åœ¨è¿›è¡Œæ¢¯åº¦ä¸‹é™ä¹‹åï¼Œv<sub>w</sub>ä¸u<sub>w</sub>ä¼šå˜å¾—ååˆ†ç›¸ä¼¼ï¼Œå› æ­¤ä¸€èˆ¬ç›´æ¥å–ä»–ä»¬çš„**å¹³å‡**ï¼Œä½œä¸ºæ”¹è¯çš„**è¯å‘é‡**

Problemsï¼š

1. è®¡ç®—é‡å¾ˆå¤§ï¼Œå› ä¸ºJ(Î¸)æ¶‰åŠæ•´ä¸ªè¯­æ–™åº“
2. éš¾ä»¥èµ°å‡ºéç‚¹/å±€éƒ¨æœ€ä¼˜ç‚¹

#### solution: Stochastic gradient descent(SGD) éšæœºæ¢¯åº¦ä¸‹é™

- åœ¨ä¸€ä¸ªå°çš„batché‡Œæ›´æ–°ï¼Œå³åœ¨æ¯ä¸€ä¸ªbatchä¸­ï¼Œè¯­æ–™åº“åªåŒ…å«çª—å£å†…çš„æ‰€æœ‰è¯ 



## 2.SGNS:Skip-gram Negative sampling


$$
J(\theta) = \frac{1}{T}âˆ‘^T_{t=1}J_t(\theta)
$$

$$
J_t(\theta) = log \sigma(u_o^Tv_c) + âˆ‘^k_{i=1}E_{j\sim P(w)}[log\sigma(-u_j^Tv_c)]
$$

$$
\sigma(x) = \frac{1}{1+e^{-x}}
$$

use word provabilites to choose k negative samples

- P(w): word probabilities

- k: number of nagative samples

å¯¹äºæ¯ä¸ªæ­£ä¾‹ï¼ˆä¸­å¤®è¯è¯­åŠä¸Šä¸‹æ–‡ä¸­çš„ä¸€ä¸ªè¯è¯­ï¼‰é‡‡æ ·å‡ ä¸ªè´Ÿä¾‹ï¼ˆä¸­å¤®è¯è¯­å’Œå…¶ä»–éšæœºè¯è¯­ï¼‰ï¼Œè®­ç»ƒbinary logistic regressionã€‚

ä½¿å¾—æœ€å¤§åŒ–ä¸­å¤®è¯ä¸ä¸Šä¸‹æ–‡çš„ç›¸å…³æ¦‚ç‡ï¼Œæœ€å°åŒ–ä¸å…¶ä»–è¯è¯­çš„æ¦‚ç‡

é—®é¢˜ï¼š

- [x] ä¸ºä»€ä¹ˆå¯¹äºä¸€ä¸ªä¸­å¿ƒè¯åªé€‰æ‹©ä¸€ä¸ªæ­£æ ·æœ¬å‘¢ï¼Œmaybeç®€åŒ–çš„å¾ˆå‰å®³ï¼Ÿç…ç…ä»£ç 
  - åªèƒ½è¯´ä»£ç é‡Œé¢ä¹Ÿç¡®å®åªæœ‰ä¸€ä¸ªæ­£ä¾‹

# Language Modeling

Assigns probability to a piece of text:
$$
P(x^{(1)},...,x^{(T)})=P(x^{(1)})*P(x^{(2)}|x^{(1)})*...P(x^{(T)}|x^{(T-1)},...,x^{(1)})
=\prod^T_{t=1}P(x^{(t)}|x^{(t-1)},...,x^{(1)})
$$

## N-gram Language Models

#### 4-gram Language Model/ 3rd order Markov Model

$$
P(w_4|w_1w_2w_3) = \frac{count(w_1w_2w_3w_4)}{count(w_1w_2w_3)}
$$

#### Sparsity Problem ï¼š

1.åˆ†å­å‡ºç°æ¬¡æ•°ä¸º0

- smoothingï¼š add small count  ğ›… for every w âˆˆ V

2.åˆ†æ¯å‡ºç°æ¬¡æ•°ä¸º0

- backoff:  å°†4-gramæ”¹ä¸º3-gramç›´åˆ°bi-gram

#### Storage Preambles 

Need to store count for all n-grams in the corpus

## RNN: Recurrent 

![img](https://mewtiger-1311904225.cos.ap-nanjing.myqcloud.com/liman/202210251129294.webp)

Advantages:

1. Can process **any length** input
2. Computation for step t can use information from **many steps back**(in theory)
3. Model size **doesn`t increase** for longer input context
4. Same weights applied on every timestep, so there is **symmetry** in how inputs are processed

Disadvantages:

1. Recurrent computation is **slow**
2. In practice, difficilt to access information from **many steps back**
