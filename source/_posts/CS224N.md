---
title: CS224N
date: 2022-10-23 18:30:54
categories: NLP
tags: [大四上,CS224N,NLP]
cover: https://mewtiger-1311904225.cos.ap-nanjing.myqcloud.com/liman/202210251150254.png

---

# Word Vectors

## Word2vec: prediction function



## 1.Skip-gram

已知当前词语，预测上下文

### **L(θ)**:	<u>date Likelihood</u> of the capacity of predicting words in context 

**m**:  size of predicting window

**w<sub>t</sub>**: given center word 

对于每一个位置t，给定窗口内每个词w<sub>t+j</sub>预测出它的概率的累乘
$$
L(\theta)=∏_{t=1}^{T}∏_{-m≤j≤m,j≠0}P(w_{t+j}|w_t;\theta)
$$


### **J(θ):**  <u>objective function / loss function</u>

$$
J(\theta)=-\frac{1}{T}logL(\theta)=-\frac{1}{T}∑_{t=1}^{T}∑_{-m≤j≤m,j≠0}logP(w_{t+j}|w_t;\theta)
$$

累乘求最大值 =》累加求最小值



### Softmax Function——<u>Scale</u>

- **max**: amplify  the probability of larger x<sub>i</sub>
- **soft**: still assign some probability of smaller x<sub>i</sub>

$$
softmax(x) = \frac{exp(x)}{∑^n_{j=1}exp(x_j)}
$$



### Prediction Function

use two vectors per word **w** to simplify <u>math and optimization</u> and can be <u>built easily</u>.

- **v<sub>w</sub>**: vector for **center** word w
- **u<sub>w</sub>**: vector of **context** word w

**c** : center word

**o** : context word
$$
P(o|c) = \frac{exp(u_o^Tv_c)}{∑_{w∈V}exp(u^T_wv_c)}
$$
**u<sup>T</sup><sub>o</sub>v<sub>c</sub>** : **similarity** of o and c

除法为了归一化



### Gradient Descent

$$
\theta^{new} = \theta^{old} - \alpha \Delta_\theta J(\theta)
$$

在进行梯度下降之后，v<sub>w</sub>与u<sub>w</sub>会变得十分相似，因此一般直接取他们的**平均**，作为改词的**词向量**

Problems：

1. 计算量很大，因为J(θ)涉及整个语料库
2. 难以走出鞍点/局部最优点

#### solution: Stochastic gradient descent(SGD) 随机梯度下降

- 在一个小的batch里更新，即在每一个batch中，语料库只包含窗口内的所有词 



## 2.SGNS:Skip-gram Negative sampling


$$
J(\theta) = \frac{1}{T}∑^T_{t=1}J_t(\theta)
$$

$$
J_t(\theta) = log \sigma(u_o^Tv_c) + ∑^k_{i=1}E_{j\sim P(w)}[log\sigma(-u_j^Tv_c)]
$$

$$
\sigma(x) = \frac{1}{1+e^{-x}}
$$

use word provabilites to choose k negative samples

- P(w): word probabilities

- k: number of nagative samples

对于每个正例（中央词语及上下文中的一个词语）采样几个负例（中央词语和其他随机词语），训练binary logistic regression。

使得最大化中央词与上下文的相关概率，最小化与其他词语的概率

问题：

- [x] 为什么对于一个中心词只选择一个正样本呢，maybe简化的很厉害？瞅瞅代码
  - 只能说代码里面也确实只有一个正例

# Language Modeling

Assigns probability to a piece of text:
$$
P(x^{(1)},...,x^{(T)})=P(x^{(1)})*P(x^{(2)}|x^{(1)})*...P(x^{(T)}|x^{(T-1)},...,x^{(1)})
=\prod^T_{t=1}P(x^{(t)}|x^{(t-1)},...,x^{(1)})
$$

## N-gram Language Models

#### 4-gram Language Model/ 3rd order Markov Model

$$
P(w_4|w_1w_2w_3) = \frac{count(w_1w_2w_3w_4)}{count(w_1w_2w_3)}
$$

#### Sparsity Problem ：

1.分子出现次数为0

- smoothing： add small count  𝛅 for every w ∈ V

2.分母出现次数为0

- backoff:  将4-gram改为3-gram直到bi-gram

#### Storage Preambles 

Need to store count for all n-grams in the corpus

# RNN: Recurrent Neural Networks

![img](https://mewtiger-1311904225.cos.ap-nanjing.myqcloud.com/liman/202210251129294.webp)

Advantages:

1. Can process **any length** input
2. Computation for step t can use information from **many steps back**(in theory)
3. Model size **doesn`t increase** for longer input context
4. Same weights applied on every timestep, so there is **symmetry** in how inputs are processed

Disadvantages:

1. Recurrent computation is **slow**
2. In practice, difficilt to access information from **many steps back**

## Train

### Loss

$$
Loss = J(\theta)=\frac{1}{T}\sum^T_{t=1}J^{(t)}(\theta)
$$

SGD can be used：such as compute loss J(θ) for a sentence

### Backpropagation

$$
\frac{\rm d J^{(t)}}{\rm d W_h} = \sum^{t}_{i=1}\frac{\rm d J^{(t)}}{\rm d W_h}|_{(i)}
$$

The gradident a repeated weight is the sum of the gradient each time it appears![Screenshot 2022-10-25 at 17.00.50](https://mewtiger-1311904225.cos.ap-nanjing.myqcloud.com/liman/202210251700405.png)

### Vanishing and Exploding Gradients

梯度消失：

- 起因于链式法则中的某些导数过小，导致梯度过小
- 导致模型无法学习到距离较远的知识

梯度爆炸：

- 梯度过大，导致越过了收敛点

## Generating

1. Give a begin words/token to RNN
2. RNN will generate text by repeated sampling

![Screenshot 2022-10-25 at 16.26.37](https://mewtiger-1311904225.cos.ap-nanjing.myqcloud.com/liman/202210251626535.png)

## Evalutate

Standard **evaluation metric** for Language Model is **perplexity**

- **lower** is better

$$
perplexity =
\prod^T_{t=1}(\frac{1}{P_{LM}(x^{(t+1)}|x^{(t)},...,x^{(1)})})^{1/T}
$$

$$
=\prod^T_{t=1}(\frac{1}{\hat{y}^{t}_{x_{t+1}}})^{1/T}
=exp(J(\theta))
$$

如果perplexity=7，则对于预测出的结果的不确定性为：

- 掷出一个7面的骰子，得到一个正确答案的不确定性

## Importance

1. **Benchmark task**: <u>measure our progress</u> on understanding language
2. **Subcomponent** of many NLP tasks, especially involving <u>generating</u> text or <u>estimating the probability of text</u>

## Multi-layer RNNs

1. RNNs deep on **one dimension**(over many timesteps)
2. By applying multiple RNNs can learn **higher-level features**(sentence structure, sentiment polarity) 
3. Multi-layer RNNs also called **stacked RNNs**



# LSTM: Long Short-Term Memory RNNs

a solution to the **vanishing gradients** problem

1. on step, there is a <u>hiiden state</u> **h<sup>(t)</sup>** and a <u>cell state</u> **c<sup>(t)</sup>**

   - Both are vectors length **n**
   - The cell stores **long-term information** 

   - The LSTM can <u>read</u>, <u>erase</u>, and <u>write</u> information from the cell(like RAM)

2. Selection of which information is erased/writen/read is controlled by three corresponding gates

   - Both are vectors length **n**
   - Each timestrp, each element of the gates can be <u>open(1),</u> <u>closed(0)</u>, or somewhere in-between.
   - Gates are **dynamic**: value is computed based on the current context

![Screenshot 2022-10-26 at 14.30.14](https://mewtiger-1311904225.cos.ap-nanjing.myqcloud.com/liman/202210261430351.png)

Key: 新的information通过**加**的方式累计在cell中，而不是乘

### summary:

1. LSTM makes it **easier** for the RNN to **preserve information over many timesteps**
2. LSTM **doesn't guarantee** that there is no vanishing/exploding gradient
3. LSTM只能学习到到之前的信息，因此可以再使用一个**独立的反向**LSTM进行叠加

# NMT:  Neural Machine Translation-The seq2seq model

![Screenshot 2022-10-26 at 15.23.30](https://mewtiger-1311904225.cos.ap-nanjing.myqcloud.com/liman/202210261523466.png)

##  Train

optimized as a **single system**. Backpropagation operates "end-to-end"

![Screenshot 2022-10-26 at 16.23.13](https://mewtiger-1311904225.cos.ap-nanjing.myqcloud.com/liman/202210261623428.png)



## Greedy Decoding

In decode, always generate target sentence by taking **argmax** on **each stop**.

Problem: 

1. 当前最优的迭代未必是target
2. no way to undo decisions

## Fix: Beam search decoding

On each step of decoder, keep track of the <u>k</u> most probable partial translations(which call **hypotheses**)

- k is the **beam size**(in practice around 5 to 10)

- Beam search is **not guaranteed** to find optimal solution ,but **much efficient** than <u>exhaustive search</u>

### Example: 

Beam size  = k = 2
$$
score(y_1,....y_t)=\sum^t_{i=1}logP_{LM}(y_i|y_1,....,y_{i-1},x)
$$


![Screenshot 2022-10-26 at 16.44.49](https://mewtiger-1311904225.cos.ap-nanjing.myqcloud.com/liman/202210261644661.png)

For each of the k hypotheses, find <u>top k next words</u> and calculate scores.

### Stop Criterion

In **greedy decoding**, stop when the model produces an \<END> token

In **beam serach decoding**, different hypotheses may produce \<END> token on **different timesteps**

- When a hypothesis produce \<END> token means it`s **complete**, **place it aside** and continue exploring other hypotheses via beam search

**Stop when**:

1. We reach timesteps T(pre-defined cutoff), or
2. We have at least n completed hypotheses(pre-defined cutoff)

### Finishing up

#### Problem: 

Each hypothesis y<sub>1</sub> ,..., y<sub>t</sub> on list has a score, but **longer hypotheses have lower scores** 

#### Fix: 

Normalize by length
$$
\frac{1}{t}\sum^t_{i=1}logP_{LM}(y_i|y_1,....,y_{i-1},x)=score_h/length_h
$$

## Evaluate: BLUE(Bilingual Evaluation Understudy)

BLEU conpares the <u>machine-written-translation</u> to one or serveral <u>human-written-translation(s)</u>, and conpute a **similarity score** based on:

1. **n-gram precision**(usually for 1, 2, 3 and 4-grams)

2. Plus a penalty for too-short system translations

BlUE is **useful** but **imperfect**

- There`re **many valid ways** to translate a sentence, so a **good** translation can get a **poor** BLEU score  because it has a low n-grams overlap with the human translation

## Difficulties

1. **OOV**
2. **Domain mismatch** between train and test data
3. Maintaining **context** over long text
4. **Low-resource** language pairs
5. Failures to accurately capture **sentence meaning**
6. **Pronoun**(or **zero pronoun**) **resolution** errors (Chinese)
7. **Morphological agreement** errors(form of words and phrases)

# Self-Attention and Transformers

![Screenshot 2022-10-26 at 18.48.26](https://mewtiger-1311904225.cos.ap-nanjing.myqcloud.com/liman/202210261855366.png)
