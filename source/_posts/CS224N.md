---
title: CS224N
date: 2022-10-23 18:30:54
categories: NLP
tags: [大四上,CS224N,NLP]
cover: https://mewtiger-1311904225.cos.ap-nanjing.myqcloud.com/liman/202210251150254.png

---

# Word Vectors

## Word2vec: prediction function



## 1.Skip-gram

已知当前词语，预测上下文

### **L(θ)**:	<u>date Likelihood</u> of the capacity of predicting words in context 

**m**:  size of predicting window

**w<sub>t</sub>**: given center word 

对于每一个位置t，给定窗口内每个词w<sub>t+j</sub>预测出它的概率的累乘
$$
L(\theta)=∏_{t=1}^{T}∏_{-m≤j≤m,j≠0}P(w_{t+j}|w_t;\theta)
$$


### **J(θ):**  <u>objective function / loss function</u>

$$
J(\theta)=-\frac{1}{T}logL(\theta)=-\frac{1}{T}∑_{t=1}^{T}∑_{-m≤j≤m,j≠0}logP(w_{t+j}|w_t;\theta)
$$

累乘求最大值 =》累加求最小值



### Softmax Function——<u>Scale</u>

- **max**: amplify  the probability of larger x<sub>i</sub>
- **soft**: still assign some probability of smaller x<sub>i</sub>

$$
softmax(x) = \frac{exp(x)}{∑^n_{j=1}exp(x_j)}
$$



### Prediction Function

use two vectors per word **w** to simplify <u>math and optimization</u> and can be <u>built easily</u>.

- **v<sub>w</sub>**: vector for **center** word w
- **u<sub>w</sub>**: vector of **context** word w

**c** : center word

**o** : context word
$$
P(o|c) = \frac{exp(u_o^Tv_c)}{∑_{w∈V}exp(u^T_wv_c)}
$$
**u<sup>T</sup><sub>o</sub>v<sub>c</sub>** : **similarity** of o and c

除法为了归一化



### Gradient Descent

$$
\theta^{new} = \theta^{old} - \alpha \Delta_\theta J(\theta)
$$

在进行梯度下降之后，v<sub>w</sub>与u<sub>w</sub>会变得十分相似，因此一般直接取他们的**平均**，作为改词的**词向量**

Problems：

1. 计算量很大，因为J(θ)涉及整个语料库
2. 难以走出鞍点/局部最优点

#### solution: Stochastic gradient descent(SGD) 随机梯度下降

- 在一个小的batch里更新，即在每一个batch中，语料库只包含窗口内的所有词 



## 2.SGNS:Skip-gram Negative sampling


$$
J(\theta) = \frac{1}{T}∑^T_{t=1}J_t(\theta)
$$

$$
J_t(\theta) = log \sigma(u_o^Tv_c) + ∑^k_{i=1}E_{j\sim P(w)}[log\sigma(-u_j^Tv_c)]
$$

$$
\sigma(x) = \frac{1}{1+e^{-x}}
$$

use word provabili		tes to choose k negative samples

- P(w): word probabilities

- k: number of nagative samples

对于每个正例（中央词语及上下文中的一个词语）采样几个负例（中央词语和其他随机词语），训练binary logistic regression。

使得最大化中央词与上下文的相关概率，最小化与其他词语的概率

问题：

- [x] 为什么对于一个中心词只选择一个正样本呢，maybe简化的很厉害？瞅瞅代码
  - 只能说代码里面也确实只有一个正例

# Language Modeling

Assigns probability to a piece of text:
$$
P(x^{(1)},...,x^{(T)})=P(x^{(1)})*P(x^{(2)}|x^{(1)})*...P(x^{(T)}|x^{(T-1)},...,x^{(1)})
=\prod^T_{t=1}P(x^{(t)}|x^{(t-1)},...,x^{(1)})
$$

## N-gram Language Models

#### 4-gram Language Model/ 3rd order Markov Model

$$
P(w_4|w_1w_2w_3) = \frac{count(w_1w_2w_3w_4)}{count(w_1w_2w_3)}
$$

#### Sparsity Problem ：

1.分子出现次数为0

- smoothing： add small count  𝛅 for every w ∈ V

2.分母出现次数为0

- backoff:  将4-gram改为3-gram直到bi-gram

#### Storage Preambles 

Need to store count for all n-grams in the corpus

# RNN: Recurrent Neural Networks

![img](https://mewtiger-1311904225.cos.ap-nanjing.myqcloud.com/liman/202210251129294.webp)

Advantages:

1. Can process **any length** input
2. Computation for step t can use information from **many steps back**(in theory)
3. Model size **doesn`t increase** for longer input context
4. Same weights applied on every timestep, so there is **symmetry** in how inputs are processed

Disadvantages:

1. Recurrent computation is **slow**
2. In practice, difficilt to access information from **many steps back**

## Train

### Loss

$$
Loss = J(\theta)=\frac{1}{T}\sum^T_{t=1}J^{(t)}(\theta)
$$

SGD can be used：such as compute loss J(θ) for a sentence

### Backpropagation

$$
\frac{\rm d J^{(t)}}{\rm d W_h} = \sum^{t}_{i=1}\frac{\rm d J^{(t)}}{\rm d W_h}|_{(i)}
$$

The gradident a repeated weight is the sum of the gradient each time it appears![Screenshot 2022-10-25 at 17.00.50](https://mewtiger-1311904225.cos.ap-nanjing.myqcloud.com/liman/202210251700405.png)

### Vanishing and Exploding Gradients

梯度消失：

- 起因于链式法则中的某些导数过小，导致梯度过小
- 导致模型无法学习到距离较远的知识

梯度爆炸：

- 梯度过大，导致越过了收敛点

## Generating

1. Give a begin words/token to RNN
2. RNN will generate text by repeated sampling

![Screenshot 2022-10-25 at 16.26.37](https://mewtiger-1311904225.cos.ap-nanjing.myqcloud.com/liman/202210251626535.png)

## Evalutate

Standard **evaluation metric** for Language Model is **perplexity**

- **lower** is better

$$
perplexity =
\prod^T_{t=1}(\frac{1}{P_{LM}(x^{(t+1)}|x^{(t)},...,x^{(1)})})^{1/T}
$$

$$
=\prod^T_{t=1}(\frac{1}{\hat{y}^{t}_{x_{t+1}}})^{1/T}
=exp(J(\theta))
$$

如果perplexity=7，则对于预测出的结果的不确定性为：

- 掷出一个7面的骰子，得到一个正确答案的不确定性

## Importance

1. **Benchmark task**: <u>measure our progress</u> on understanding language
2. **Subcomponent** of many NLP tasks, especially involving <u>generating</u> text or <u>estimating the probability of text</u>

## Multi-layer RNNs

1. RNNs deep on **one dimension**(over many timesteps)
2. By applying multiple RNNs can learn **higher-level features**(sentence structure, sentiment polarity) 
3. Multi-layer RNNs also called **stacked RNNs**



# LSTM: Long Short-Term Memory RNNs

a solution to the **vanishing gradients** problem

1. on step, there is a <u>hiiden state</u> **h<sup>(t)</sup>** and a <u>cell state</u> **c<sup>(t)</sup>**

   - Both are vectors length **n**
   - The cell stores **long-term information** 

   - The LSTM can <u>read</u>, <u>erase</u>, and <u>write</u> information from the cell(like RAM)

2. Selection of which information is erased/writen/read is controlled by three corresponding gates

   - Both are vectors length **n**
   - Each timestrp, each element of the gates can be <u>open(1),</u> <u>closed(0)</u>, or somewhere in-between.
   - Gates are **dynamic**: value is computed based on the current context

![Screenshot 2022-10-26 at 14.30.14](https://mewtiger-1311904225.cos.ap-nanjing.myqcloud.com/liman/202210261430351.png)

Key: 新的information通过**加**的方式累计在cell中，而不是乘

### summary:

1. LSTM makes it **easier** for the RNN to **preserve information over many timesteps**
2. LSTM **doesn't guarantee** that there is no vanishing/exploding gradient
3. LSTM只能学习到到之前的信息，因此可以再使用一个**独立的反向**LSTM进行叠加

# NMT:  Neural Machine Translation-The seq2seq model

![Screenshot 2022-10-26 at 15.23.30](https://mewtiger-1311904225.cos.ap-nanjing.myqcloud.com/liman/202210261523466.png)

##  Train

optimized as a **single system**. Backpropagation operates "end-to-end"

![Screenshot 2022-10-26 at 16.23.13](https://mewtiger-1311904225.cos.ap-nanjing.myqcloud.com/liman/202210261623428.png)



## Greedy Decoding

In decode, always generate target sentence by taking **argmax** on **each stop**.

Problem: 

1. 当前最优的迭代未必是target
2. no way to undo decisions

## Fix: Beam search decoding

On each step of decoder, keep track of the <u>k</u> most probable partial translations(which call **hypotheses**)

- k is the **beam size**(in practice around 5 to 10)

- Beam search is **not guaranteed** to find optimal solution ,but **much efficient** than <u>exhaustive search</u>

### Example: 

Beam size  = k = 2
$$
score(y_1,....y_t)=\sum^t_{i=1}logP_{LM}(y_i|y_1,....,y_{i-1},x)
$$


![Screenshot 2022-10-26 at 16.44.49](https://mewtiger-1311904225.cos.ap-nanjing.myqcloud.com/liman/202210261644661.png)

For each of the k hypotheses, find <u>top k next words</u> and calculate scores.

### Stop Criterion

In **greedy decoding**, stop when the model produces an \<END> token

In **beam serach decoding**, different hypotheses may produce \<END> token on **different timesteps**

- When a hypothesis produce \<END> token means it`s **complete**, **place it aside** and continue exploring other hypotheses via beam search

**Stop when**:

1. We reach timesteps T(pre-defined cutoff), or
2. We have at least n completed hypotheses(pre-defined cutoff)

### Finishing up

#### Problem: 

Each hypothesis y<sub>1</sub> ,..., y<sub>t</sub> on list has a score, but **longer hypotheses have lower scores** 

#### Fix: 

Normalize by length
$$
\frac{1}{t}\sum^t_{i=1}logP_{LM}(y_i|y_1,....,y_{i-1},x)=score_h/length_h
$$

## Evaluate: BLUE(Bilingual Evaluation Understudy)

BLEU conpares the <u>machine-written-translation</u> to one or serveral <u>human-written-translation(s)</u>, and conpute a **similarity score** based on:

1. **n-gram precision**(usually for 1, 2, 3 and 4-grams)

2. Plus a penalty for too-short system translations

BlUE is **useful** but **imperfect**

- There`re **many valid ways** to translate a sentence, so a **good** translation can get a **poor** BLEU score  because it has a low n-grams overlap with the human translation

## Difficulties

1. **OOV**
2. **Domain mismatch** between train and test data
3. Maintaining **context** over long text
4. **Low-resource** language pairs
5. Failures to accurately capture **sentence meaning**
6. **Pronoun**(or **zero pronoun**) **resolution** errors (Chinese)
7. **Morphological agreement** errors(form of words and phrases)

# Self-Attention and Transformers

## Self-Attention

### RNN的**问题**：

1. Linear interaction distance: **O(sequence length)**
2. Lack of parallelizability

#### **Fix**: Word window

- 对于每一个embedding提供一个word window以计算context，并消去timestep上的依赖
- 当word window size = 5 时，第一层可以覆盖长度为5的上下文，二层可以覆盖长度为9的上下文
- 但当sequence长度过长时，依旧会失去远距离的context



![Screenshot 2022-10-27 at 10.12.36](https://mewtiger-1311904225.cos.ap-nanjing.myqcloud.com/liman/202210271012970.png)

### **改进**：Self-Attention

- In self-attention, the **queries** q<sub>i</sub>, **keys** k<sub>i</sub>, **values** v<sub>i</sub> are drawn from the same source
- Self-attention operation is as follows:

$$
e_{ij}=q_i^Tk_i
$$

$$
\alpha_{ij}=\frac{exp(e_{ij})}{\sum_{j'}exp(e_{ij'})}
$$

$$
output_i = \sum_j\alpha_{ij}v_j
$$

![在这里插入图片描述](https://mewtiger-1311904225.cos.ap-nanjing.myqcloud.com/liman/202210271057391.png)

#### **优势**：相比全长度word window

**dynamic connectivity** ： 

1. 全连接层的权重是在训练中迭代学习，学到是你应该注意哪些神经元，但对于不同的句子有不同的结构，所需要注意的神经元也不同
2. self-attention是queries和keys的点积，依赖于实际的文本

**interaction**：

1. 全连接层中，各部分是独立的连接在一起，没有与当前query交互，也没有其他的key的交互

### Barriers and Solutions for Self-Attention as a building block

#### 1.Doesn't have an inherent notion of **order**

##### ⓵Postion representation vectors through **sinusoids**

Pros:

- Periodicity indicates that maybe <u>"absolute position" isn't as important</u>
- Maybe can <u>extrapolate to longer sequence</u> as period restart

Cons:

- Not learnable

##### ⓶Postion representation vectors learned from scratch: Learn a **matrix**

Pros:

- **Flexibility**: each position gets to be learned to fit the data

Cons:

- Definityle **can`t extrapolate** to indices outside 1,...T.

#### 2.No nonlinearities for deep learning magic

Just apply the same **feedforward network** to each self-attention output

#### 3.Need to ensure we don`t "look at future" when predicting

Masking the future in self-attention by **setting attention scores**
$$
e_{ij} = \begin{cases} q_i^Tk_j, i<j \\ -\infty,i≥j \end{cases}
$$

## Transformers (what's more )

### Key-Query-Value Attention

x<sub>i</sub>,...,x<sub>T</sub> are input vectors to the Transformer encoder, x<sub>i</sub> ∈ R<sup>d</sup>

- k<sub>i</sub> = Kx<sub>i</sub>, K  ∈ R<sup>dxd</sup>
- q<sub>i</sub> = Qx<sub>i</sub>, Q  ∈ R<sup>dxd</sup>
- v<sub>i</sub> = Vx<sub>i</sub>, V  ∈ R<sup>dxd</sup>

**Calculate**:

Let X = [x<sub>1</sub>; ... ; x<sub>T</sub> ] ∈ R<sup>Txd</sup>
$$
output = softmax(XQ(XK)^T)\times XV
$$
![Screenshot 2022-10-27 at 11.41.01](https://mewtiger-1311904225.cos.ap-nanjing.myqcloud.com/liman/202210271141647.png)

### Multi-headed attention

look in multiple places in the sentence at once
$$
Let \quad Q_l,K_l,V_l ∈ R^{d\times\frac{d}{h}}
$$
**h** is the number of attention heads, and **l** ranges from 1 to h
$$
output_l = softmax(XQ_l(XK_l)^T)\times XV_l, output_l∈R^{d/h}
$$

$$
output = Y[output_1,..., output_h], Y∈R^{d\times d}
$$

![Screenshot 2022-10-27 at 12.14.29](https://mewtiger-1311904225.cos.ap-nanjing.myqcloud.com/liman/202210271214475.png)

> 并不会增加实际的运算量

## Training Tricks:

### Residual Connections

Help models train **better**

- Instead of X<sup>(i)</sup> = Layer(X<sup>(i-1)</sup>),i represents the layer
- Let  **X<sup>(i)</sup> = X<sup>(i-1)</sup> + Layer(X<sup>(i-1)</sup>)**

### Layer Normalization

Help models train **faster**

![Screenshot 2022-10-27 at 12.36.34](https://mewtiger-1311904225.cos.ap-nanjing.myqcloud.com/liman/202210271236244.png)

### Scale Dot Product

当向量维度过大，点积就很容易变得非常大，导致softmax变得**peaky**，从而导致其他其余梯度过小
$$
output_l = softmax(\frac{XQ_l(XK_l)^T}{\sqrt{d/h}})\times XV_l, output_l∈R^{d/h}
$$

## Cross-attention

The **keys** and **values** are drawn from the **encoder**(like a memory)

The **queries** are drawn from the **decoder**

- Let H = [h<sub>1</sub>; ... ; h<sub>T</sub> ] ∈ R<sup>Txd</sup>
- Let Z = [z<sub>1</sub>; ... ; z<sub>T</sub> ] ∈ R<sup>Txd</sup>

<u>k<sub>i</sub> = Kh<sub>i</sub> , V<sub>i</sub> = Vh<sub>i</sub> , q<sub>i</sub> = Qz<sub>i</sub></u> 
$$
output = softmax(ZQ(HK)^T)\times HV
$$

