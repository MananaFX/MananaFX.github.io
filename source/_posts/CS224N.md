---
title: CS224N
date: 2022-10-23 18:30:54
categories: NLP
tags: [å¤§å››ä¸Š,CS224N,NLP]
cover: https://mewtiger-1311904225.cos.ap-nanjing.myqcloud.com/liman/202210251150254.png

---

# Word Vectors

## Word2vec: prediction function



## 1.Skip-gram

å·²çŸ¥å½“å‰è¯è¯­ï¼Œé¢„æµ‹ä¸Šä¸‹æ–‡

### **L(Î¸)**:	<u>date Likelihood</u> of the capacity of predicting words in context 

**m**:  size of predicting window

**w<sub>t</sub>**: given center word 

å¯¹äºæ¯ä¸€ä¸ªä½ç½®tï¼Œç»™å®šçª—å£å†…æ¯ä¸ªè¯w<sub>t+j</sub>é¢„æµ‹å‡ºå®ƒçš„æ¦‚ç‡çš„ç´¯ä¹˜
$$
L(\theta)=âˆ_{t=1}^{T}âˆ_{-mâ‰¤jâ‰¤m,jâ‰ 0}P(w_{t+j}|w_t;\theta)
$$


### **J(Î¸):**  <u>objective function / loss function</u>

$$
J(\theta)=-\frac{1}{T}logL(\theta)=-\frac{1}{T}âˆ‘_{t=1}^{T}âˆ‘_{-mâ‰¤jâ‰¤m,jâ‰ 0}logP(w_{t+j}|w_t;\theta)
$$

ç´¯ä¹˜æ±‚æœ€å¤§å€¼ =ã€‹ç´¯åŠ æ±‚æœ€å°å€¼



### Softmax Functionâ€”â€”<u>Scale</u>

- **max**: amplify  the probability of larger x<sub>i</sub>
- **soft**: still assign some probability of smaller x<sub>i</sub>

$$
softmax(x) = \frac{exp(x)}{âˆ‘^n_{j=1}exp(x_j)}
$$



### Prediction Function

use two vectors per word **w** to simplify <u>math and optimization</u> and can be <u>built easily</u>.

- **v<sub>w</sub>**: vector for **center** word w
- **u<sub>w</sub>**: vector of **context** word w

**c** : center word

**o** : context word
$$
P(o|c) = \frac{exp(u_o^Tv_c)}{âˆ‘_{wâˆˆV}exp(u^T_wv_c)}
$$
**u<sup>T</sup><sub>o</sub>v<sub>c</sub>** : **similarity** of o and c

é™¤æ³•ä¸ºäº†å½’ä¸€åŒ–



### Gradient Descent

$$
\theta^{new} = \theta^{old} - \alpha \Delta_\theta J(\theta)
$$

åœ¨è¿›è¡Œæ¢¯åº¦ä¸‹é™ä¹‹åï¼Œv<sub>w</sub>ä¸u<sub>w</sub>ä¼šå˜å¾—ååˆ†ç›¸ä¼¼ï¼Œå› æ­¤ä¸€èˆ¬ç›´æ¥å–ä»–ä»¬çš„**å¹³å‡**ï¼Œä½œä¸ºæ”¹è¯çš„**è¯å‘é‡**

Problemsï¼š

1. è®¡ç®—é‡å¾ˆå¤§ï¼Œå› ä¸ºJ(Î¸)æ¶‰åŠæ•´ä¸ªè¯­æ–™åº“
2. éš¾ä»¥èµ°å‡ºéç‚¹/å±€éƒ¨æœ€ä¼˜ç‚¹

#### solution: Stochastic gradient descent(SGD) éšæœºæ¢¯åº¦ä¸‹é™

- åœ¨ä¸€ä¸ªå°çš„batché‡Œæ›´æ–°ï¼Œå³åœ¨æ¯ä¸€ä¸ªbatchä¸­ï¼Œè¯­æ–™åº“åªåŒ…å«çª—å£å†…çš„æ‰€æœ‰è¯ 



## 2.SGNS:Skip-gram Negative sampling


$$
J(\theta) = \frac{1}{T}âˆ‘^T_{t=1}J_t(\theta)
$$

$$
J_t(\theta) = log \sigma(u_o^Tv_c) + âˆ‘^k_{i=1}E_{j\sim P(w)}[log\sigma(-u_j^Tv_c)]
$$

$$
\sigma(x) = \frac{1}{1+e^{-x}}
$$

use word provabili		tes to choose k negative samples

- P(w): word probabilities

- k: number of nagative samples

å¯¹äºæ¯ä¸ªæ­£ä¾‹ï¼ˆä¸­å¤®è¯è¯­åŠä¸Šä¸‹æ–‡ä¸­çš„ä¸€ä¸ªè¯è¯­ï¼‰é‡‡æ ·å‡ ä¸ªè´Ÿä¾‹ï¼ˆä¸­å¤®è¯è¯­å’Œå…¶ä»–éšæœºè¯è¯­ï¼‰ï¼Œè®­ç»ƒbinary logistic regressionã€‚

ä½¿å¾—æœ€å¤§åŒ–ä¸­å¤®è¯ä¸ä¸Šä¸‹æ–‡çš„ç›¸å…³æ¦‚ç‡ï¼Œæœ€å°åŒ–ä¸å…¶ä»–è¯è¯­çš„æ¦‚ç‡

é—®é¢˜ï¼š

- [x] ä¸ºä»€ä¹ˆå¯¹äºä¸€ä¸ªä¸­å¿ƒè¯åªé€‰æ‹©ä¸€ä¸ªæ­£æ ·æœ¬å‘¢ï¼Œmaybeç®€åŒ–çš„å¾ˆå‰å®³ï¼Ÿç…ç…ä»£ç 
  - åªèƒ½è¯´ä»£ç é‡Œé¢ä¹Ÿç¡®å®åªæœ‰ä¸€ä¸ªæ­£ä¾‹

# Language Modeling

Assigns probability to a piece of text:
$$
P(x^{(1)},...,x^{(T)})=P(x^{(1)})*P(x^{(2)}|x^{(1)})*...P(x^{(T)}|x^{(T-1)},...,x^{(1)})
=\prod^T_{t=1}P(x^{(t)}|x^{(t-1)},...,x^{(1)})
$$

## N-gram Language Models

#### 4-gram Language Model/ 3rd order Markov Model

$$
P(w_4|w_1w_2w_3) = \frac{count(w_1w_2w_3w_4)}{count(w_1w_2w_3)}
$$

#### Sparsity Problem ï¼š

1.åˆ†å­å‡ºç°æ¬¡æ•°ä¸º0

- smoothingï¼š add small count  ğ›… for every w âˆˆ V

2.åˆ†æ¯å‡ºç°æ¬¡æ•°ä¸º0

- backoff:  å°†4-gramæ”¹ä¸º3-gramç›´åˆ°bi-gram

#### Storage Preambles 

Need to store count for all n-grams in the corpus

# RNN: Recurrent Neural Networks

![img](https://mewtiger-1311904225.cos.ap-nanjing.myqcloud.com/liman/202210251129294.webp)

Advantages:

1. Can process **any length** input
2. Computation for step t can use information from **many steps back**(in theory)
3. Model size **doesn`t increase** for longer input context
4. Same weights applied on every timestep, so there is **symmetry** in how inputs are processed

Disadvantages:

1. Recurrent computation is **slow**
2. In practice, difficilt to access information from **many steps back**

## Train

### Loss

$$
Loss = J(\theta)=\frac{1}{T}\sum^T_{t=1}J^{(t)}(\theta)
$$

SGD can be usedï¼šsuch as compute loss J(Î¸) for a sentence

### Backpropagation

$$
\frac{\rm d J^{(t)}}{\rm d W_h} = \sum^{t}_{i=1}\frac{\rm d J^{(t)}}{\rm d W_h}|_{(i)}
$$

The gradident a repeated weight is the sum of the gradient each time it appears![Screenshot 2022-10-25 at 17.00.50](https://mewtiger-1311904225.cos.ap-nanjing.myqcloud.com/liman/202210251700405.png)

### Vanishing and Exploding Gradients

æ¢¯åº¦æ¶ˆå¤±ï¼š

- èµ·å› äºé“¾å¼æ³•åˆ™ä¸­çš„æŸäº›å¯¼æ•°è¿‡å°ï¼Œå¯¼è‡´æ¢¯åº¦è¿‡å°
- å¯¼è‡´æ¨¡å‹æ— æ³•å­¦ä¹ åˆ°è·ç¦»è¾ƒè¿œçš„çŸ¥è¯†

æ¢¯åº¦çˆ†ç‚¸ï¼š

- æ¢¯åº¦è¿‡å¤§ï¼Œå¯¼è‡´è¶Šè¿‡äº†æ”¶æ•›ç‚¹

## Generating

1. Give a begin words/token to RNN
2. RNN will generate text by repeated sampling

![Screenshot 2022-10-25 at 16.26.37](https://mewtiger-1311904225.cos.ap-nanjing.myqcloud.com/liman/202210251626535.png)

## Evalutate

Standard **evaluation metric** for Language Model is **perplexity**

- **lower** is better

$$
perplexity =
\prod^T_{t=1}(\frac{1}{P_{LM}(x^{(t+1)}|x^{(t)},...,x^{(1)})})^{1/T}
$$

$$
=\prod^T_{t=1}(\frac{1}{\hat{y}^{t}_{x_{t+1}}})^{1/T}
=exp(J(\theta))
$$

å¦‚æœperplexity=7ï¼Œåˆ™å¯¹äºé¢„æµ‹å‡ºçš„ç»“æœçš„ä¸ç¡®å®šæ€§ä¸ºï¼š

- æ·å‡ºä¸€ä¸ª7é¢çš„éª°å­ï¼Œå¾—åˆ°ä¸€ä¸ªæ­£ç¡®ç­”æ¡ˆçš„ä¸ç¡®å®šæ€§

## Importance

1. **Benchmark task**: <u>measure our progress</u> on understanding language
2. **Subcomponent** of many NLP tasks, especially involving <u>generating</u> text or <u>estimating the probability of text</u>

## Multi-layer RNNs

1. RNNs deep on **one dimension**(over many timesteps)
2. By applying multiple RNNs can learn **higher-level features**(sentence structure, sentiment polarity) 
3. Multi-layer RNNs also called **stacked RNNs**



# LSTM: Long Short-Term Memory RNNs

a solution to the **vanishing gradients** problem

1. on step, there is a <u>hiiden state</u> **h<sup>(t)</sup>** and a <u>cell state</u> **c<sup>(t)</sup>**

   - Both are vectors length **n**
   - The cell stores **long-term information** 

   - The LSTM can <u>read</u>, <u>erase</u>, and <u>write</u> information from the cell(like RAM)

2. Selection of which information is erased/writen/read is controlled by three corresponding gates

   - Both are vectors length **n**
   - Each timestrp, each element of the gates can be <u>open(1),</u> <u>closed(0)</u>, or somewhere in-between.
   - Gates are **dynamic**: value is computed based on the current context

![Screenshot 2022-10-26 at 14.30.14](https://mewtiger-1311904225.cos.ap-nanjing.myqcloud.com/liman/202210261430351.png)

Key: æ–°çš„informationé€šè¿‡**åŠ **çš„æ–¹å¼ç´¯è®¡åœ¨cellä¸­ï¼Œè€Œä¸æ˜¯ä¹˜

### summary:

1. LSTM makes it **easier** for the RNN to **preserve information over many timesteps**
2. LSTM **doesn't guarantee** that there is no vanishing/exploding gradient
3. LSTMåªèƒ½å­¦ä¹ åˆ°åˆ°ä¹‹å‰çš„ä¿¡æ¯ï¼Œå› æ­¤å¯ä»¥å†ä½¿ç”¨ä¸€ä¸ª**ç‹¬ç«‹çš„åå‘**LSTMè¿›è¡Œå åŠ 

# NMT:  Neural Machine Translation-The seq2seq model

![Screenshot 2022-10-26 at 15.23.30](https://mewtiger-1311904225.cos.ap-nanjing.myqcloud.com/liman/202210261523466.png)

##  Train

optimized as a **single system**. Backpropagation operates "end-to-end"

![Screenshot 2022-10-26 at 16.23.13](https://mewtiger-1311904225.cos.ap-nanjing.myqcloud.com/liman/202210261623428.png)



## Greedy Decoding

In decode, always generate target sentence by taking **argmax** on **each stop**.

Problem: 

1. å½“å‰æœ€ä¼˜çš„è¿­ä»£æœªå¿…æ˜¯target
2. no way to undo decisions

## Fix: Beam search decoding

On each step of decoder, keep track of the <u>k</u> most probable partial translations(which call **hypotheses**)

- k is the **beam size**(in practice around 5 to 10)

- Beam search is **not guaranteed** to find optimal solution ,but **much efficient** than <u>exhaustive search</u>

### Example: 

Beam size  = k = 2
$$
score(y_1,....y_t)=\sum^t_{i=1}logP_{LM}(y_i|y_1,....,y_{i-1},x)
$$


![Screenshot 2022-10-26 at 16.44.49](https://mewtiger-1311904225.cos.ap-nanjing.myqcloud.com/liman/202210261644661.png)

For each of the k hypotheses, find <u>top k next words</u> and calculate scores.

### Stop Criterion

In **greedy decoding**, stop when the model produces an \<END> token

In **beam serach decoding**, different hypotheses may produce \<END> token on **different timesteps**

- When a hypothesis produce \<END> token means it`s **complete**, **place it aside** and continue exploring other hypotheses via beam search

**Stop when**:

1. We reach timesteps T(pre-defined cutoff), or
2. We have at least n completed hypotheses(pre-defined cutoff)

### Finishing up

#### Problem: 

Each hypothesis y<sub>1</sub> ,..., y<sub>t</sub> on list has a score, but **longer hypotheses have lower scores** 

#### Fix: 

Normalize by length
$$
\frac{1}{t}\sum^t_{i=1}logP_{LM}(y_i|y_1,....,y_{i-1},x)=score_h/length_h
$$

## Evaluate: BLUE(Bilingual Evaluation Understudy)

BLEU conpares the <u>machine-written-translation</u> to one or serveral <u>human-written-translation(s)</u>, and conpute a **similarity score** based on:

1. **n-gram precision**(usually for 1, 2, 3 and 4-grams)

2. Plus a penalty for too-short system translations

BlUE is **useful** but **imperfect**

- There`re **many valid ways** to translate a sentence, so a **good** translation can get a **poor** BLEU score  because it has a low n-grams overlap with the human translation

## Difficulties

1. **OOV**
2. **Domain mismatch** between train and test data
3. Maintaining **context** over long text
4. **Low-resource** language pairs
5. Failures to accurately capture **sentence meaning**
6. **Pronoun**(or **zero pronoun**) **resolution** errors (Chinese)
7. **Morphological agreement** errors(form of words and phrases)

# Self-Attention and Transformers

## Self-Attention

### RNNçš„**é—®é¢˜**ï¼š

1. Linear interaction distance: **O(sequence length)**
2. Lack of parallelizability

#### **Fix**: Word window

- å¯¹äºæ¯ä¸€ä¸ªembeddingæä¾›ä¸€ä¸ªword windowä»¥è®¡ç®—contextï¼Œå¹¶æ¶ˆå»timestepä¸Šçš„ä¾èµ–
- å½“word window size = 5 æ—¶ï¼Œç¬¬ä¸€å±‚å¯ä»¥è¦†ç›–é•¿åº¦ä¸º5çš„ä¸Šä¸‹æ–‡ï¼ŒäºŒå±‚å¯ä»¥è¦†ç›–é•¿åº¦ä¸º9çš„ä¸Šä¸‹æ–‡
- ä½†å½“sequenceé•¿åº¦è¿‡é•¿æ—¶ï¼Œä¾æ—§ä¼šå¤±å»è¿œè·ç¦»çš„context



![Screenshot 2022-10-27 at 10.12.36](https://mewtiger-1311904225.cos.ap-nanjing.myqcloud.com/liman/202210271012970.png)

### **æ”¹è¿›**ï¼šSelf-Attention

- In self-attention, the **queries** q<sub>i</sub>, **keys** k<sub>i</sub>, **values** v<sub>i</sub> are drawn from the same source
- Self-attention operation is as follows:

$$
e_{ij}=q_i^Tk_i
$$

$$
\alpha_{ij}=\frac{exp(e_{ij})}{\sum_{j'}exp(e_{ij'})}
$$

$$
output_i = \sum_j\alpha_{ij}v_j
$$

![åœ¨è¿™é‡Œæ’å…¥å›¾ç‰‡æè¿°](https://mewtiger-1311904225.cos.ap-nanjing.myqcloud.com/liman/202210271057391.png)

#### **ä¼˜åŠ¿**ï¼šç›¸æ¯”å…¨é•¿åº¦word window

**dynamic connectivity** ï¼š 

1. å…¨è¿æ¥å±‚çš„æƒé‡æ˜¯åœ¨è®­ç»ƒä¸­è¿­ä»£å­¦ä¹ ï¼Œå­¦åˆ°æ˜¯ä½ åº”è¯¥æ³¨æ„å“ªäº›ç¥ç»å…ƒï¼Œä½†å¯¹äºä¸åŒçš„å¥å­æœ‰ä¸åŒçš„ç»“æ„ï¼Œæ‰€éœ€è¦æ³¨æ„çš„ç¥ç»å…ƒä¹Ÿä¸åŒ
2. self-attentionæ˜¯querieså’Œkeysçš„ç‚¹ç§¯ï¼Œä¾èµ–äºå®é™…çš„æ–‡æœ¬

**interaction**ï¼š

1. å…¨è¿æ¥å±‚ä¸­ï¼Œå„éƒ¨åˆ†æ˜¯ç‹¬ç«‹çš„è¿æ¥åœ¨ä¸€èµ·ï¼Œæ²¡æœ‰ä¸å½“å‰queryäº¤äº’ï¼Œä¹Ÿæ²¡æœ‰å…¶ä»–çš„keyçš„äº¤äº’

### Barriers and Solutions for Self-Attention as a building block

#### 1.Doesn't have an inherent notion of **order**

##### â“µPostion representation vectors through **sinusoids**

Pros:

- Periodicity indicates that maybe <u>"absolute position" isn't as important</u>
- Maybe can <u>extrapolate to longer sequence</u> as period restart

Cons:

- Not learnable

##### â“¶Postion representation vectors learned from scratch: Learn a **matrix**

Pros:

- **Flexibility**: each position gets to be learned to fit the data

Cons:

- Definityle **can`t extrapolate** to indices outside 1,...T.

#### 2.No nonlinearities for deep learning magic

Just apply the same **feedforward network** to each self-attention output

#### 3.Need to ensure we don`t "look at future" when predicting

Masking the future in self-attention by **setting attention scores**
$$
e_{ij} = \begin{cases} q_i^Tk_j, i<j \\ -\infty,iâ‰¥j \end{cases}
$$

## Transformers (what's more )

### Key-Query-Value Attention

x<sub>i</sub>,...,x<sub>T</sub> are input vectors to the Transformer encoder, x<sub>i</sub> âˆˆ R<sup>d</sup>

- k<sub>i</sub> = Kx<sub>i</sub>, K  âˆˆ R<sup>dxd</sup>
- q<sub>i</sub> = Qx<sub>i</sub>, Q  âˆˆ R<sup>dxd</sup>
- v<sub>i</sub> = Vx<sub>i</sub>, V  âˆˆ R<sup>dxd</sup>

**Calculate**:

Let X = [x<sub>1</sub>; ... ; x<sub>T</sub> ] âˆˆ R<sup>Txd</sup>
$$
output = softmax(XQ(XK)^T)\times XV
$$
![Screenshot 2022-10-27 at 11.41.01](https://mewtiger-1311904225.cos.ap-nanjing.myqcloud.com/liman/202210271141647.png)

### Multi-headed attention

look in multiple places in the sentence at once
$$
Let \quad Q_l,K_l,V_l âˆˆ R^{d\times\frac{d}{h}}
$$
**h** is the number of attention heads, and **l** ranges from 1 to h
$$
output_l = softmax(XQ_l(XK_l)^T)\times XV_l, output_lâˆˆR^{d/h}
$$

$$
output = Y[output_1,..., output_h], YâˆˆR^{d\times d}
$$

![Screenshot 2022-10-27 at 12.14.29](https://mewtiger-1311904225.cos.ap-nanjing.myqcloud.com/liman/202210271214475.png)

> å¹¶ä¸ä¼šå¢åŠ å®é™…çš„è¿ç®—é‡

## Training Tricks:

### Residual Connections

Help models train **better**

- Instead of X<sup>(i)</sup> = Layer(X<sup>(i-1)</sup>),i represents the layer
- Let  **X<sup>(i)</sup> = X<sup>(i-1)</sup> + Layer(X<sup>(i-1)</sup>)**

### Layer Normalization

Help models train **faster**

![Screenshot 2022-10-27 at 12.36.34](https://mewtiger-1311904225.cos.ap-nanjing.myqcloud.com/liman/202210271236244.png)

### Scale Dot Product

å½“å‘é‡ç»´åº¦è¿‡å¤§ï¼Œç‚¹ç§¯å°±å¾ˆå®¹æ˜“å˜å¾—éå¸¸å¤§ï¼Œå¯¼è‡´softmaxå˜å¾—**peaky**ï¼Œä»è€Œå¯¼è‡´å…¶ä»–å…¶ä½™æ¢¯åº¦è¿‡å°
$$
output_l = softmax(\frac{XQ_l(XK_l)^T}{\sqrt{d/h}})\times XV_l, output_lâˆˆR^{d/h}
$$

## Cross-attention

The **keys** and **values** are drawn from the **encoder**(like a memory)

The **queries** are drawn from the **decoder**

- Let H = [h<sub>1</sub>; ... ; h<sub>T</sub> ] âˆˆ R<sup>Txd</sup>
- Let Z = [z<sub>1</sub>; ... ; z<sub>T</sub> ] âˆˆ R<sup>Txd</sup>

<u>k<sub>i</sub> = Kh<sub>i</sub> , V<sub>i</sub> = Vh<sub>i</sub> , q<sub>i</sub> = Qz<sub>i</sub></u> 
$$
output = softmax(ZQ(HK)^T)\times HV
$$

